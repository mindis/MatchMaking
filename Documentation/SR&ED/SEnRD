1. What technological uncertainties/challenges did you encounter in 2018? (An uncertainty/challenge in this case is when the standard approach/method to resolve a problem either did not work or could not work)



With the objective of personalizing a user experience with the accuracy of 80% or above, machine learning techniques were introduced to our product. During our research and proof of concepts, multiple machine learning methods were evaluated. We tried out the K-Nearest Neighbors method, followed by Collaborative Filtering methods such as Singular Value Decomposition and Matrix Factorization with Alternating Least Squares, followed by optimization method Stochastic Gradient Descent.

As a result of our testing, we discovered that the standard machine learning algorithms does not align with our needs due to the fact that the way user search results are returned in our home buying product require a technology that can make real-time personalized recommendations based on user characteristics, behavior, and demography in conjunction with current market trends, property inventory, location popularity, and real estate regulations. While there were standard algorithms to use such data sets as a standalone, there was no standard approach to aggregate all these different data sets into an algorithm (or a set of algorithms) to return quality insights. 





2. Why were those standard approaches/methods not effective?

Standard Machine Learning approach was not effective due to the following reasons:-

a) Standard Alternative Least Square with Spark MLib technique was used to train a machine learning model to deliver the personalized user experience. After testing the model with different values for regularizing parameter, Root Mean Square Error (RMSE) was 2.14576 on testing data set with 3 features and lambda=0.08. These results were not effective due to the fact that the algorithm uses Collaborative Filtering approach which is limited to use only user behavioral data. 


b) Another common method known as K-nearest neighbor was tested. This method uses a description of the item and a profile of the user’s preferences. 
We used distance metric like Euclidean, Jaccard similarity to find k most similar items to a particular instance of an item. We started by working on a library called Surprise implemented in Python. After trying multiple combinations of tuning parameters, we found that using k = 4 neighbors gave better results. However, After running multiple instances of the KNN algorithm, we quickly found that the Root-Mean-Square-Error we obtained was always slightly higher than 2. For this reason, we stopped pursuing the KNN route. 




3. To overcome your various challenges/uncertainties, what did you try that didn’t work (failed attempts)?


Following are the various techniques that were tried and failed:

a) We used optimization technique known as Stochastic Gradient Descent to minimize the RMSE error (Cost function). We derived a gradient each with respect to user and property data and used lambda =0.02. We repeated the process up until convergence of the training Root Mean Squared Error. The process was very slow so we added more compute power by adding more resources. However, there were no significant improvements. 



b) Additionally, We went backward to refine our data set further by redoing feature engineering. For example, we used Natural Language Processing (NLP) techniques to convert property description into multiple smaller numerical features, we also used one-hot encoding, log transformation, etc to further refine our data sets so that algorithm can return optimal results. However, the results were far from satisfactory as the user to property hit ratio was still less than 50%. 
