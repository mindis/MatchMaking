{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>9</td><td>application_1564494171557_0010</td><td>pyspark</td><td>idle</td><td></td><td></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "<SparkContext master=yarn appName=livy-session-9>"
     ]
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "#import seaborn as sns\n",
    "#import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.sql import SQLContext,functions as F\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "g_appname=\"nest_spark_get_mongodb\"\n",
    "\n",
    "#Local\n",
    "#g_input=\"mongodb://127.0.0.1/backend_production.properties\"\n",
    "\n",
    "#Production\n",
    "#g_input=\"mongodb://10.0.1.70:27017/backend_production.properties\"\n",
    "\n",
    "g_input=\"mongodb://3.210.155.32:27017/backend_production.properties\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "my_spark = SparkSession \\\n",
    "     .builder \\\n",
    "     .appName(\"amazon-personalize-items-dataset\") \\\n",
    "     .config(\"spark.mongodb.input.uri\", g_input ) \\\n",
    "     .config(\"spark.sql.pivotMaxValues\", 100000) \\\n",
    "     .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.11:2.4.0')\\\n",
    "     .config('spark.driver.memory', \"10g\")\\\n",
    "     .config('spark.executor.memory', \"8g\")\\\n",
    "     .config(\"spark.executor.cores\",\"4\")\\\n",
    "     .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#pipeline=\"[{'$match': {'status':'active', 'country': 'CA' }},{ $project : {'beds':1,'price_cents':1} } ]\"\n",
    "\n",
    "#pipeline=\"[{'$match': {'status':'active', 'country': 'CA' }},{ $project : {'beds':1,'price_cents':1,'city':1,'postal':1,'created_at':1,'status':1} } ]\"\n",
    "\n",
    "#pipeline=\"[{'$match': {'external_type':'list_hub', 'status':'active','country': 'CA' }},{ $project : {'beds':1,'price_cents':1,'city':1,'postal':1,'created_at':1,'status':1,'country':1} } ]\"\n",
    "\n",
    "pipeline=\"[{'$match': {'status':'active'}},{ $project : {'beds':1,'price_cents':1,'city':1,'postal':1,'created_at':1,'status':1,'country':1} } ]\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_SCHEMA=StructType(  (\n",
    "                StructField('_id',StringType(),True),\n",
    "                StructField('price_cents',IntegerType(),True),\n",
    "                StructField('beds',IntegerType(),True),\n",
    "                StructField('city',StringType(),True),\n",
    "                StructField('postal',StringType(),True),\n",
    "                StructField('status',StringType(),True),\n",
    "                StructField('created_at',TimestampType(),True),\n",
    "                StructField('country',StringType(),True),\n",
    "               )\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.clearCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\")\\\n",
    "    .option(\"pipeline\", pipeline)\\\n",
    "    .schema(P_SCHEMA)\\\n",
    "    .option(\"inferSchema\",\"false\")\\\n",
    "    .load()\n",
    "    #.limit(5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: string (nullable = true)\n",
      " |-- price_cents: integer (nullable = true)\n",
      " |-- beds: integer (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- postal: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- created_at: timestamp (nullable = true)\n",
      " |-- country: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.registerTempTable(\"df_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_spark.catalog.cacheTable(\"df_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function unix_time at 0x7f6b833fb5f0>"
     ]
    }
   ],
   "source": [
    "import time\n",
    "def unix_time(val):\n",
    "    return int(time.mktime(time.strptime(val, '%Y-%m-%d')))\n",
    "\n",
    "spark.udf.register(\"unix_time\", unix_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filter = sqlContext.sql('''select _id as ITEM_ID, \n",
    "                                        NVL(price_cents,0) as PRICE, \n",
    "                                        NVL(beds,0) as BEDS,\n",
    "                                        NVL(city,'UNKNOWN') as CITY,\n",
    "                                        NVL(postal,'UNKNOWN') as POSTAL,\n",
    "                                        unix_time(substr(created_at,1,10)) as CREATED_AT,\n",
    "                                        country\n",
    "                                        from df_tbl\n",
    "                                        order by 6 asc\n",
    "                            ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ITEM_ID: string (nullable = true)\n",
      " |-- PRICE: integer (nullable = false)\n",
      " |-- BEDS: integer (nullable = false)\n",
      " |-- CITY: string (nullable = false)\n",
      " |-- POSTAL: string (nullable = false)\n",
      " |-- CREATED_AT: string (nullable = true)\n",
      " |-- country: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "df_filter.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "781"
     ]
    }
   ],
   "source": [
    "#df_filter.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filter.repartition(1) \\\n",
    "        .write.format(\"com.databricks.spark.csv\") \\\n",
    "        .partitionBy('country')\\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"header\",\"true\")\\\n",
    "        .save(\"s3://nestready-amazon-personalize/items/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ca/items/part-00000-6c04e1b1-ebfd-4328-bca4-5da4b7c17c5c-c000.csv\n",
    "##ca/items/part-00000-66a7cb5a-3e05-4e27-8d92-f7e8cf83e557-c000.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_spark.catalog.uncacheTable(\"df_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
