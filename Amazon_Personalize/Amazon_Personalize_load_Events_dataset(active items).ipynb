{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>3</td><td>application_1566222863010_0004</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-0-1-28.ec2.internal:20888/proxy/application_1566222863010_0004/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-0-1-27.ec2.internal:8042/node/containerlogs/container_1566222863010_0004_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "<SparkContext master=yarn appName=livy-session-3>"
     ]
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>4</td><td>application_1566222863010_0005</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-0-1-28.ec2.internal:20888/proxy/application_1566222863010_0005/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-0-1-27.ec2.internal:8042/node/containerlogs/container_1566222863010_0005_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'driverMemory': '10240M', 'executorMemory': '5120M', 'executorCores': 4, 'conf': {'spark.pyspark.python': 'python', 'livy.server.session.timeout-check': 'false', 'livy.server.session.timeout': 7200000}, 'proxyUser': 'jovyan', 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>2</td><td>application_1566222863010_0003</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-0-1-28.ec2.internal:20888/proxy/application_1566222863010_0003/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-0-1-6.ec2.internal:8042/node/containerlogs/container_1566222863010_0003_01_000001/livy\">Link</a></td><td></td></tr><tr><td>4</td><td>application_1566222863010_0005</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-0-1-28.ec2.internal:20888/proxy/application_1566222863010_0005/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-0-1-27.ec2.internal:8042/node/containerlogs/container_1566222863010_0005_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\"driverMemory\": \"10240M\", \"executorMemory\": \"5120M\", \"executorCores\": 4, \"conf\":{\"spark.pyspark.python\": \"python\",\"livy.server.session.timeout-check\": \"false\" , \"livy.server.session.timeout\": 7200000 }}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.sql import SQLContext,functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_1=spark.read.json(\"s3://nsegmentlogs-v3/*/\").filter( col(\"propertyId\").isNotNull() & \n",
    "                                                             col(\"userId\").isNotNull()\n",
    "                                                    )     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_na=spark.read.json(\"s3://nsegmentlogs-v3-na/*/\").filter( col(\"propertyId\").isNotNull()\n",
    "                                                    )     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "dfs = [df_1,df_na]\n",
    "df = reduce(DataFrame.unionAll, dfs)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40546"
     ]
    }
   ],
   "source": [
    "#df_1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2477"
     ]
    }
   ],
   "source": [
    "#df_na.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge both datasets\n",
    "df_1.registerTempTable(\"df_tbl\")\n",
    "df_na.registerTempTable(\"df_na_tbl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1499126400"
     ]
    }
   ],
   "source": [
    "#int(time.mktime(time.strptime('2017-07-4', '%Y-%m-%d')))\n",
    "\n",
    "#int(time.mktime(time.strptime(eventTimestamp, '%Y%m%d%H%M%S')))\n",
    "\n",
    "#2019-06-30T03:10:38.493Z\n",
    "\n",
    "#1504137600\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def ttime(val):\n",
    "    return int(time.mktime(time.strptime(val, '%Y%m%d%H%M%S')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function ttime at 0x7f1c1e7ddb90>"
     ]
    }
   ],
   "source": [
    "spark.udf.register(\"ttime\", ttime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_union=spark.sql('''\n",
    "select userId as USER_ID, propertyId as ITEM_ID, replace(propertyCountry,'Canada','CA') as COUNTRY, ttime(eventTimestamp) as TIMESTAMP from df_tbl\n",
    "UNION ALL\n",
    "select NVL(NVL(contactId.Id, userId),1) as USER_ID, propertyId as ITEM_ID, replace(propertyCountry,'Canada','CA') as COUNTRY, ttime(eventTimestamp) as TIMESTAMP from df_na_tbl\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- USER_ID: string (nullable = true)\n",
      " |-- ITEM_ID: string (nullable = true)\n",
      " |-- COUNTRY: string (nullable = true)\n",
      " |-- TIMESTAMP: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "df_union.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_union.registerTempTable(\"df_union_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.cacheTable(\"df_union_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37832"
     ]
    }
   ],
   "source": [
    "#df_union.count()\n",
    "# July 10 - 36604\n",
    "# July 16 - 37832"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from bson.objectid import ObjectId\n",
    "import pandas as pd\n",
    "#g_input=\"mongodb://10.0.1.70\"\n",
    "\n",
    "g_input=\"mongodb://3.210.155.32\"\n",
    "\n",
    "client = MongoClient(g_input,27017)\n",
    "\n",
    "\n",
    "db = client.backend_production\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_pd = pd.DataFrame(columns=['propertyId'])\n",
    "l_cnt=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df_union.select('ITEM_ID').distinct().collect():\n",
    "    cnt= db.properties.find( {'_id': ObjectId(i[0]), 'status': 'active'}).count()\n",
    "    #cnt= db.properties.find( {'_id': ObjectId(i[0])}).count()\n",
    "    if cnt == 0:\n",
    "        l_cnt=l_cnt+1\n",
    "        df_pd.loc[l_cnt] = [i[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7231, 1)"
     ]
    }
   ],
   "source": [
    "df_pd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = sqlContext.createDataFrame(df_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df.registerTempTable(\"spark_df_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_active=spark.sql('''\n",
    "select * from df_union_tbl a where 1=1 and not exists (select 1 from spark_df_tbl b where a.ITEM_ID=b.propertyId)\n",
    "\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_active.registerTempTable(\"df_active_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|country|count(1)|\n",
      "+-------+--------+\n",
      "|   null|     182|\n",
      "|     CA|    2026|\n",
      "|     US|    8095|\n",
      "+-------+--------+"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "select country, count(*) from df_active_tbl group by 1\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|country|count(1)|\n",
      "+-------+--------+\n",
      "|   null|     197|\n",
      "|     CA|    3265|\n",
      "|     US|    9461|\n",
      "+-------+--------+"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "select country, count(*) from df_active_tbl group by 1\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------------------+\n",
      "|country|count(DISTINCT item_id, user_id)|\n",
      "+-------+--------------------------------+\n",
      "|   null|                              33|\n",
      "|     CA|                             704|\n",
      "|     US|                            2057|\n",
      "+-------+--------------------------------+"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "select country, count(distinct item_id, user_id) from df_active_tbl group by 1\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------------------+\n",
      "|country|count(DISTINCT item_id, user_id)|\n",
      "+-------+--------------------------------+\n",
      "|   null|                              48|\n",
      "|     CA|                            1275|\n",
      "|     US|                            2823|\n",
      "+-------+--------------------------------+"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "select country, count(distinct item_id, user_id) from df_active_tbl group by 1\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------------------------------+\n",
      "|country|count(DISTINCT item_id, user_id, timestamp)|\n",
      "+-------+-------------------------------------------+\n",
      "|   null|                                         55|\n",
      "|     CA|                                       1208|\n",
      "|     US|                                       4089|\n",
      "+-------+-------------------------------------------+"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "select country, count(distinct item_id, user_id, timestamp) from df_active_tbl group by 1\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------------------------------+\n",
      "|country|count(DISTINCT item_id, user_id, timestamp)|\n",
      "+-------+-------------------------------------------+\n",
      "|   null|                                         68|\n",
      "|     CA|                                       2444|\n",
      "|     US|                                       5492|\n",
      "+-------+-------------------------------------------+"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "select country, count(distinct item_id, user_id, timestamp) from df_active_tbl group by 1\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_active_filter=spark.sql('''\n",
    "select distinct user_id, item_id, country, timestamp from df_active_tbl\n",
    "''').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_active_filter=df_active.select('user_id', 'item_id', 'country', 'timestamp').distinct()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_active_filter.registerTempTable(\"df_active_filter_tbl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5352"
     ]
    }
   ],
   "source": [
    "df_active_filter.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_active_filter.repartition(1) \\\n",
    "        .write.format(\"com.databricks.spark.csv\") \\\n",
    "        .partitionBy('country')\\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"header\",\"true\")\\\n",
    "        .save(\"s3://nestready-amazon-personalize/events-active\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.uncacheTable(\"df_union_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Filter old items from metadata so that coldstart items remain less than 80K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_items_events= spark.read.load(\"s3://nestready-amazon-personalize/events-active/country=US/\", \n",
    "                      format='com.databricks.spark.csv', \n",
    "                      header='true', \n",
    "                      inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4089"
     ]
    }
   ],
   "source": [
    "df_items_events.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_items_metdata = spark.read.load(\"s3://nestready-amazon-personalize/items/country=US/\", \n",
    "                      format='com.databricks.spark.csv', \n",
    "                      header='true', \n",
    "                      inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2361107"
     ]
    }
   ],
   "source": [
    "df_items_metdata.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_items_events.registerTempTable(\"df_items_events_tbl\")\n",
    "\n",
    "df_items_metdata.registerTempTable(\"df_items_metdata_tbl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- item_id: string (nullable = true)\n",
      " |-- timestamp: integer (nullable = true)"
     ]
    }
   ],
   "source": [
    "df_items_events.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ITEM_ID: string (nullable = true)\n",
      " |-- PRICE: integer (nullable = true)\n",
      " |-- BEDS: integer (nullable = true)\n",
      " |-- CITY: string (nullable = true)\n",
      " |-- POSTAL: string (nullable = true)\n",
      " |-- CREATED_AT: integer (nullable = true)"
     ]
    }
   ],
   "source": [
    "df_items_metdata.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_unix_format(p_dt):\n",
    "    import datetime\n",
    "    dt = datetime.datetime.fromtimestamp(p_dt).strftime('%Y-%m-%d')\n",
    "    return dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1559211010"
     ]
    }
   ],
   "source": [
    "print (int(time.mktime(time.strptime('20190530101010', '%Y%m%d%H%M%S'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.sql('''\n",
    "select * from df_items_metdata_tbl a\n",
    "where 1=1\n",
    "and (exists (select 1 from df_items_events_tbl b where b.item_id=a.item_id)\n",
    "OR CREATED_AT > 1559174400\n",
    ")\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.repartition(1) \\\n",
    "        .write.format(\"com.databricks.spark.csv\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"header\",\"true\")\\\n",
    "        .save(\"s3://nestready-amazon-personalize/items-filtered/US/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
