{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "from collections import defaultdict\n",
    "#import seaborn as sns\n",
    "#import matplotlib.pyplot as plt\n",
    "import json\n",
    "from bson import json_util\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.sql import SQLContext,functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#global variables\n",
    "\n",
    "g_appname=\"nest_spark_get_mongodb\"\n",
    "g_master=\"local[2]\"\n",
    "g_memory=\"30g\"\n",
    "\n",
    "#Local\n",
    "g_input=\"mongodb://127.0.0.1/backend_production.properties\"\n",
    "\n",
    "#Production\n",
    "#g_input=\"mongodb://3.210.155.32/backend_production.properties\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### set Spark session\n",
    "my_spark = SparkSession \\\n",
    "     .builder \\\n",
    "     .appName(g_appname) \\\n",
    "     .master(g_master) \\\n",
    "     .config('spark.driver.memory', g_memory)\\\n",
    "     .config(\"spark.mongodb.input.uri\", g_input ) \\\n",
    "     .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.11:2.4.0')\\\n",
    "     .getOrCreate()\n",
    "\n",
    "\n",
    "     #.config(\"spark.mongodb.output.uri\", \"mongodb://3.210.155.32/backend_production.properties\") \\\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set SQL Context\n",
    "sqlContext=SQLContext(my_spark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "df = my_spark.read.format(\"com.mongodb.spark.sql.DefaultSource\")\\\n",
    "    .option(\"database\",\"backend_production\")\\\n",
    "    .option(\"collection\", \"properties\")\\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nest_db_properties_spark import get_mongo_data_spark\n",
    "#df=get_mongo_data_spark()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- address_number: string (nullable = true)\n",
      " |-- address_street: string (nullable = true)\n",
      " |-- agent_avatar_url: null (nullable = true)\n",
      " |-- agent_email: string (nullable = true)\n",
      " |-- agent_external_id: string (nullable = true)\n",
      " |-- agent_fname: string (nullable = true)\n",
      " |-- agent_id: string (nullable = true)\n",
      " |-- agent_key: string (nullable = true)\n",
      " |-- agent_lname: string (nullable = true)\n",
      " |-- agent_name: string (nullable = true)\n",
      " |-- agent_phone: string (nullable = true)\n",
      " |-- architecture_style: string (nullable = true)\n",
      " |-- available_showing: null (nullable = true)\n",
      " |-- basement: boolean (nullable = true)\n",
      " |-- bathrooms: double (nullable = true)\n",
      " |-- beds: integer (nullable = true)\n",
      " |-- brokerage: struct (nullable = true)\n",
      " |    |-- address: struct (nullable = true)\n",
      " |    |    |-- commons:City: string (nullable = true)\n",
      " |    |    |-- commons:Country: string (nullable = true)\n",
      " |    |    |-- commons:FullStreetAddress: string (nullable = true)\n",
      " |    |    |-- commons:PostalCode: string (nullable = true)\n",
      " |    |    |-- commons:StateOrProvince: string (nullable = true)\n",
      " |    |    |-- commons:UnitNumber: string (nullable = true)\n",
      " |    |    |-- commons:address-preference-order: string (nullable = true)\n",
      " |    |    |-- commons:preference-order: string (nullable = true)\n",
      " |    |-- logo_url: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- phone: string (nullable = true)\n",
      " |    |-- website_url: string (nullable = true)\n",
      " |-- brokerage_email: string (nullable = true)\n",
      " |-- brokerage_external_id: string (nullable = true)\n",
      " |-- brokerage_logo_url: string (nullable = true)\n",
      " |-- brokerage_name: string (nullable = true)\n",
      " |-- brokerage_phone: string (nullable = true)\n",
      " |-- brokerage_website_url: string (nullable = true)\n",
      " |-- building_sqft: double (nullable = true)\n",
      " |-- building_type: string (nullable = true)\n",
      " |-- building_utilities: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- cooling_systems: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- cover_public_url: string (nullable = true)\n",
      " |-- created_at: timestamp (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- disclose_address: string (nullable = true)\n",
      " |-- exterior_types: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- external_id: string (nullable = true)\n",
      " |-- external_type: struct (nullable = true)\n",
      " |    |-- symbol: string (nullable = true)\n",
      " |-- external_url: string (nullable = true)\n",
      " |-- fees: null (nullable = true)\n",
      " |-- fireplaces: string (nullable = true)\n",
      " |-- floors: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- floors_number: double (nullable = true)\n",
      " |-- foreclosure: boolean (nullable = true)\n",
      " |-- full_baths: integer (nullable = true)\n",
      " |-- furnished: null (nullable = true)\n",
      " |-- garage: boolean (nullable = true)\n",
      " |-- geocoded: boolean (nullable = true)\n",
      " |-- half_baths: integer (nullable = true)\n",
      " |-- has_attic: null (nullable = true)\n",
      " |-- has_barbecue_area: boolean (nullable = true)\n",
      " |-- has_basement: boolean (nullable = true)\n",
      " |-- has_ceiling_fan: boolean (nullable = true)\n",
      " |-- has_deck: boolean (nullable = true)\n",
      " |-- has_disabled_access: boolean (nullable = true)\n",
      " |-- has_dock: boolean (nullable = true)\n",
      " |-- has_doorman: boolean (nullable = true)\n",
      " |-- has_double_pane_windows: boolean (nullable = true)\n",
      " |-- has_elevator: boolean (nullable = true)\n",
      " |-- has_fireplace: boolean (nullable = true)\n",
      " |-- has_garage: boolean (nullable = true)\n",
      " |-- has_garden: boolean (nullable = true)\n",
      " |-- has_gated_entry: boolean (nullable = true)\n",
      " |-- has_green_house: boolean (nullable = true)\n",
      " |-- has_hot_tub_spa: boolean (nullable = true)\n",
      " |-- has_jetted_bath_tub: boolean (nullable = true)\n",
      " |-- has_mother_in_law: boolean (nullable = true)\n",
      " |-- has_patio: boolean (nullable = true)\n",
      " |-- has_pond: boolean (nullable = true)\n",
      " |-- has_pool: boolean (nullable = true)\n",
      " |-- has_porch: boolean (nullable = true)\n",
      " |-- has_rv_parking: boolean (nullable = true)\n",
      " |-- has_sauna: boolean (nullable = true)\n",
      " |-- has_security_system: boolean (nullable = true)\n",
      " |-- has_skylight: boolean (nullable = true)\n",
      " |-- has_sports_court: boolean (nullable = true)\n",
      " |-- has_sprinkler_system: boolean (nullable = true)\n",
      " |-- has_vaulted_ceiling: boolean (nullable = true)\n",
      " |-- has_wet_bar: boolean (nullable = true)\n",
      " |-- heating_fuels: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- heating_systems: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- images: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- _id: struct (nullable = true)\n",
      " |    |    |    |-- oid: string (nullable = true)\n",
      " |    |    |-- created_at: timestamp (nullable = true)\n",
      " |    |    |-- modified_at: timestamp (nullable = true)\n",
      " |    |    |-- original_url: string (nullable = true)\n",
      " |    |    |-- sha1: string (nullable = true)\n",
      " |    |    |-- updated_at: timestamp (nullable = true)\n",
      " |    |    |-- url: string (nullable = true)\n",
      " |-- intercom: boolean (nullable = true)\n",
      " |-- is_cable_ready: null (nullable = true)\n",
      " |-- is_new_construction: boolean (nullable = true)\n",
      " |-- is_waterfront: boolean (nullable = true)\n",
      " |-- is_wired: boolean (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- lead_routing_email: string (nullable = true)\n",
      " |-- listing_category: string (nullable = true)\n",
      " |-- listing_date: timestamp (nullable = true)\n",
      " |-- listing_status: struct (nullable = true)\n",
      " |    |-- symbol: string (nullable = true)\n",
      " |-- listing_title: string (nullable = true)\n",
      " |-- listing_type: string (nullable = true)\n",
      " |-- listing_url: string (nullable = true)\n",
      " |-- living_area: string (nullable = true)\n",
      " |-- location: struct (nullable = true)\n",
      " |    |-- type: string (nullable = true)\n",
      " |    |-- coordinates: array (nullable = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |-- location_ids: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- oid: string (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- lot_sqft: double (nullable = true)\n",
      " |-- lower_location_id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- main_level_sqft: null (nullable = true)\n",
      " |-- mls_id: string (nullable = true)\n",
      " |-- mls_name: string (nullable = true)\n",
      " |-- mls_number: string (nullable = true)\n",
      " |-- modified_at: timestamp (nullable = true)\n",
      " |-- num_floors: double (nullable = true)\n",
      " |-- one_quarter_bathrooms: integer (nullable = true)\n",
      " |-- open_house_count: null (nullable = true)\n",
      " |-- parking: string (nullable = true)\n",
      " |-- parking_types: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- partial_bathrooms: integer (nullable = true)\n",
      " |-- participant: struct (nullable = true)\n",
      " |    |-- email: string (nullable = true)\n",
      " |    |-- first_name: string (nullable = true)\n",
      " |    |-- id: string (nullable = true)\n",
      " |    |-- key: string (nullable = true)\n",
      " |    |-- last_name: string (nullable = true)\n",
      " |    |-- office_phone: string (nullable = true)\n",
      " |    |-- primary_contact_phone: string (nullable = true)\n",
      " |    |-- website_url: string (nullable = true)\n",
      " |-- patio: boolean (nullable = true)\n",
      " |-- pool: boolean (nullable = true)\n",
      " |-- postal: string (nullable = true)\n",
      " |-- price_cents: integer (nullable = true)\n",
      " |-- price_cents_sqft: integer (nullable = true)\n",
      " |-- property_sub_type: string (nullable = true)\n",
      " |-- property_type: string (nullable = true)\n",
      " |-- provider_category: string (nullable = true)\n",
      " |-- provider_name: string (nullable = true)\n",
      " |-- provider_url: string (nullable = true)\n",
      " |-- province: string (nullable = true)\n",
      " |-- roof: string (nullable = true)\n",
      " |-- roof_types: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- room_count: integer (nullable = true)\n",
      " |-- rooms: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- school: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- scoring: null (nullable = true)\n",
      " |-- showing_date: null (nullable = true)\n",
      " |-- state: null (nullable = true)\n",
      " |-- status: struct (nullable = true)\n",
      " |    |-- symbol: string (nullable = true)\n",
      " |-- stories: null (nullable = true)\n",
      " |-- tags: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- symbol: string (nullable = true)\n",
      " |-- tax_amount: string (nullable = true)\n",
      " |-- three_quarter_bathrooms: integer (nullable = true)\n",
      " |-- total_sqft: double (nullable = true)\n",
      " |-- unit_number: string (nullable = true)\n",
      " |-- updated_at: timestamp (nullable = true)\n",
      " |-- utilities: string (nullable = true)\n",
      " |-- water: string (nullable = true)\n",
      " |-- water_heater: string (nullable = true)\n",
      " |-- year_built: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_id', 'struct<oid:string>'),\n",
       " ('address_number', 'string'),\n",
       " ('address_street', 'string'),\n",
       " ('agent_avatar_url', 'null'),\n",
       " ('agent_email', 'string'),\n",
       " ('agent_external_id', 'string'),\n",
       " ('agent_fname', 'string'),\n",
       " ('agent_id', 'string'),\n",
       " ('agent_key', 'string'),\n",
       " ('agent_lname', 'string'),\n",
       " ('agent_name', 'string'),\n",
       " ('agent_phone', 'string'),\n",
       " ('architecture_style', 'string'),\n",
       " ('available_showing', 'null'),\n",
       " ('basement', 'boolean'),\n",
       " ('bathrooms', 'double'),\n",
       " ('beds', 'int'),\n",
       " ('brokerage',\n",
       "  'struct<address:struct<commons:City:string,commons:Country:string,commons:FullStreetAddress:string,commons:PostalCode:string,commons:StateOrProvince:string,commons:UnitNumber:string,commons:address-preference-order:string,commons:preference-order:string>,logo_url:string,name:string,phone:string,website_url:string>'),\n",
       " ('brokerage_email', 'string'),\n",
       " ('brokerage_external_id', 'string'),\n",
       " ('brokerage_logo_url', 'string'),\n",
       " ('brokerage_name', 'string'),\n",
       " ('brokerage_phone', 'string'),\n",
       " ('brokerage_website_url', 'string'),\n",
       " ('building_sqft', 'double'),\n",
       " ('building_type', 'string'),\n",
       " ('building_utilities', 'string'),\n",
       " ('city', 'string'),\n",
       " ('cooling_systems', 'array<string>'),\n",
       " ('country', 'string'),\n",
       " ('cover_public_url', 'string'),\n",
       " ('created_at', 'timestamp'),\n",
       " ('description', 'string'),\n",
       " ('disclose_address', 'string'),\n",
       " ('exterior_types', 'array<string>'),\n",
       " ('external_id', 'string'),\n",
       " ('external_type', 'struct<symbol:string>'),\n",
       " ('external_url', 'string'),\n",
       " ('fees', 'null'),\n",
       " ('fireplaces', 'string'),\n",
       " ('floors', 'array<string>'),\n",
       " ('floors_number', 'double'),\n",
       " ('foreclosure', 'boolean'),\n",
       " ('full_baths', 'int'),\n",
       " ('furnished', 'null'),\n",
       " ('garage', 'boolean'),\n",
       " ('geocoded', 'boolean'),\n",
       " ('half_baths', 'int'),\n",
       " ('has_attic', 'null'),\n",
       " ('has_barbecue_area', 'boolean'),\n",
       " ('has_basement', 'boolean'),\n",
       " ('has_ceiling_fan', 'boolean'),\n",
       " ('has_deck', 'boolean'),\n",
       " ('has_disabled_access', 'boolean'),\n",
       " ('has_dock', 'boolean'),\n",
       " ('has_doorman', 'boolean'),\n",
       " ('has_double_pane_windows', 'boolean'),\n",
       " ('has_elevator', 'boolean'),\n",
       " ('has_fireplace', 'boolean'),\n",
       " ('has_garage', 'boolean'),\n",
       " ('has_garden', 'boolean'),\n",
       " ('has_gated_entry', 'boolean'),\n",
       " ('has_green_house', 'boolean'),\n",
       " ('has_hot_tub_spa', 'boolean'),\n",
       " ('has_jetted_bath_tub', 'boolean'),\n",
       " ('has_mother_in_law', 'boolean'),\n",
       " ('has_patio', 'boolean'),\n",
       " ('has_pond', 'boolean'),\n",
       " ('has_pool', 'boolean'),\n",
       " ('has_porch', 'boolean'),\n",
       " ('has_rv_parking', 'boolean'),\n",
       " ('has_sauna', 'boolean'),\n",
       " ('has_security_system', 'boolean'),\n",
       " ('has_skylight', 'boolean'),\n",
       " ('has_sports_court', 'boolean'),\n",
       " ('has_sprinkler_system', 'boolean'),\n",
       " ('has_vaulted_ceiling', 'boolean'),\n",
       " ('has_wet_bar', 'boolean'),\n",
       " ('heating_fuels', 'array<string>'),\n",
       " ('heating_systems', 'array<string>'),\n",
       " ('images',\n",
       "  'array<struct<_id:struct<oid:string>,created_at:timestamp,modified_at:timestamp,original_url:string,sha1:string,updated_at:timestamp,url:string>>'),\n",
       " ('intercom', 'boolean'),\n",
       " ('is_cable_ready', 'null'),\n",
       " ('is_new_construction', 'boolean'),\n",
       " ('is_waterfront', 'boolean'),\n",
       " ('is_wired', 'boolean'),\n",
       " ('latitude', 'double'),\n",
       " ('lead_routing_email', 'string'),\n",
       " ('listing_category', 'string'),\n",
       " ('listing_date', 'timestamp'),\n",
       " ('listing_status', 'struct<symbol:string>'),\n",
       " ('listing_title', 'string'),\n",
       " ('listing_type', 'string'),\n",
       " ('listing_url', 'string'),\n",
       " ('living_area', 'string'),\n",
       " ('location', 'struct<type:string,coordinates:array<double>>'),\n",
       " ('location_ids', 'array<struct<oid:string>>'),\n",
       " ('longitude', 'double'),\n",
       " ('lot_sqft', 'double'),\n",
       " ('lower_location_id', 'struct<oid:string>'),\n",
       " ('main_level_sqft', 'null'),\n",
       " ('mls_id', 'string'),\n",
       " ('mls_name', 'string'),\n",
       " ('mls_number', 'string'),\n",
       " ('modified_at', 'timestamp'),\n",
       " ('num_floors', 'double'),\n",
       " ('one_quarter_bathrooms', 'int'),\n",
       " ('open_house_count', 'null'),\n",
       " ('parking', 'string'),\n",
       " ('parking_types', 'array<string>'),\n",
       " ('partial_bathrooms', 'int'),\n",
       " ('participant',\n",
       "  'struct<email:string,first_name:string,id:string,key:string,last_name:string,office_phone:string,primary_contact_phone:string,website_url:string>'),\n",
       " ('patio', 'boolean'),\n",
       " ('pool', 'boolean'),\n",
       " ('postal', 'string'),\n",
       " ('price_cents', 'int'),\n",
       " ('price_cents_sqft', 'int'),\n",
       " ('property_sub_type', 'string'),\n",
       " ('property_type', 'string'),\n",
       " ('provider_category', 'string'),\n",
       " ('provider_name', 'string'),\n",
       " ('provider_url', 'string'),\n",
       " ('province', 'string'),\n",
       " ('roof', 'string'),\n",
       " ('roof_types', 'array<string>'),\n",
       " ('room_count', 'int'),\n",
       " ('rooms', 'array<string>'),\n",
       " ('school', 'array<string>'),\n",
       " ('scoring', 'null'),\n",
       " ('showing_date', 'null'),\n",
       " ('state', 'null'),\n",
       " ('status', 'struct<symbol:string>'),\n",
       " ('stories', 'null'),\n",
       " ('tags', 'array<struct<symbol:string>>'),\n",
       " ('tax_amount', 'string'),\n",
       " ('three_quarter_bathrooms', 'int'),\n",
       " ('total_sqft', 'double'),\n",
       " ('unit_number', 'string'),\n",
       " ('updated_at', 'timestamp'),\n",
       " ('utilities', 'string'),\n",
       " ('water', 'string'),\n",
       " ('water_heater', 'string'),\n",
       " ('year_built', 'int')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IntegerType"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema['year_built'].dataType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringType"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema['water_heater'].dataType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DoubleType"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema['total_sqft'].dataType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NullType"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema['stories'].dataType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BooleanType"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema['pool'].dataType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "for i in df.columns:\n",
    "    df= df.withColumn(i, df[i].cast(StringType()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StructType(List(StructField(symbol,StringType,true)))\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n",
      "StringType\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "from pyspark.sql.types import *\n",
    "for i in df.columns:\n",
    "    #df= df.withColumn(i, df[i].cast(StringType()))\n",
    "    #print(df.schema[i].dataType)\n",
    "    b=df.schema['stories'].dataType\n",
    "    \n",
    "    if ((df.schema[i].dataType == df.schema['stories'].dataType) | (df.schema[i].dataType == df.schema['total_sqft'].dataType)\\\n",
    "         | (df.schema[i].dataType == df.schema['basement'].dataType) | (df.schema[i].dataType == df.schema['brokerage'].dataType)\n",
    "         | (df.schema[i].dataType == df.schema['external_type'].dataType) \n",
    "       ):\n",
    "        print(df.schema[i].dataType)\n",
    "        df= df.withColumn(i, df[i].cast(StringType()))\n",
    "        \n",
    "'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: string (nullable = true)\n",
      " |-- address_number: string (nullable = true)\n",
      " |-- address_street: string (nullable = true)\n",
      " |-- agent_avatar_url: string (nullable = true)\n",
      " |-- agent_email: string (nullable = true)\n",
      " |-- agent_external_id: string (nullable = true)\n",
      " |-- agent_fname: string (nullable = true)\n",
      " |-- agent_id: string (nullable = true)\n",
      " |-- agent_key: string (nullable = true)\n",
      " |-- agent_lname: string (nullable = true)\n",
      " |-- agent_name: string (nullable = true)\n",
      " |-- agent_phone: string (nullable = true)\n",
      " |-- architecture_style: string (nullable = true)\n",
      " |-- available_showing: string (nullable = true)\n",
      " |-- basement: string (nullable = true)\n",
      " |-- bathrooms: string (nullable = true)\n",
      " |-- beds: string (nullable = true)\n",
      " |-- brokerage: string (nullable = true)\n",
      " |-- brokerage_email: string (nullable = true)\n",
      " |-- brokerage_external_id: string (nullable = true)\n",
      " |-- brokerage_logo_url: string (nullable = true)\n",
      " |-- brokerage_name: string (nullable = true)\n",
      " |-- brokerage_phone: string (nullable = true)\n",
      " |-- brokerage_website_url: string (nullable = true)\n",
      " |-- building_sqft: string (nullable = true)\n",
      " |-- building_type: string (nullable = true)\n",
      " |-- building_utilities: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- cooling_systems: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- cover_public_url: string (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- disclose_address: string (nullable = true)\n",
      " |-- exterior_types: string (nullable = true)\n",
      " |-- external_id: string (nullable = true)\n",
      " |-- external_type: string (nullable = true)\n",
      " |-- external_url: string (nullable = true)\n",
      " |-- fees: string (nullable = true)\n",
      " |-- fireplaces: string (nullable = true)\n",
      " |-- floors: string (nullable = true)\n",
      " |-- floors_number: string (nullable = true)\n",
      " |-- foreclosure: string (nullable = true)\n",
      " |-- full_baths: string (nullable = true)\n",
      " |-- furnished: string (nullable = true)\n",
      " |-- garage: string (nullable = true)\n",
      " |-- geocoded: string (nullable = true)\n",
      " |-- half_baths: string (nullable = true)\n",
      " |-- has_attic: string (nullable = true)\n",
      " |-- has_barbecue_area: string (nullable = true)\n",
      " |-- has_basement: string (nullable = true)\n",
      " |-- has_ceiling_fan: string (nullable = true)\n",
      " |-- has_deck: string (nullable = true)\n",
      " |-- has_disabled_access: string (nullable = true)\n",
      " |-- has_dock: string (nullable = true)\n",
      " |-- has_doorman: string (nullable = true)\n",
      " |-- has_double_pane_windows: string (nullable = true)\n",
      " |-- has_elevator: string (nullable = true)\n",
      " |-- has_fireplace: string (nullable = true)\n",
      " |-- has_garage: string (nullable = true)\n",
      " |-- has_garden: string (nullable = true)\n",
      " |-- has_gated_entry: string (nullable = true)\n",
      " |-- has_green_house: string (nullable = true)\n",
      " |-- has_hot_tub_spa: string (nullable = true)\n",
      " |-- has_jetted_bath_tub: string (nullable = true)\n",
      " |-- has_mother_in_law: string (nullable = true)\n",
      " |-- has_patio: string (nullable = true)\n",
      " |-- has_pond: string (nullable = true)\n",
      " |-- has_pool: string (nullable = true)\n",
      " |-- has_porch: string (nullable = true)\n",
      " |-- has_rv_parking: string (nullable = true)\n",
      " |-- has_sauna: string (nullable = true)\n",
      " |-- has_security_system: string (nullable = true)\n",
      " |-- has_skylight: string (nullable = true)\n",
      " |-- has_sports_court: string (nullable = true)\n",
      " |-- has_sprinkler_system: string (nullable = true)\n",
      " |-- has_vaulted_ceiling: string (nullable = true)\n",
      " |-- has_wet_bar: string (nullable = true)\n",
      " |-- heating_fuels: string (nullable = true)\n",
      " |-- heating_systems: string (nullable = true)\n",
      " |-- images: string (nullable = true)\n",
      " |-- intercom: string (nullable = true)\n",
      " |-- is_cable_ready: string (nullable = true)\n",
      " |-- is_new_construction: string (nullable = true)\n",
      " |-- is_waterfront: string (nullable = true)\n",
      " |-- is_wired: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- lead_routing_email: string (nullable = true)\n",
      " |-- listing_category: string (nullable = true)\n",
      " |-- listing_date: string (nullable = true)\n",
      " |-- listing_status: string (nullable = true)\n",
      " |-- listing_title: string (nullable = true)\n",
      " |-- listing_type: string (nullable = true)\n",
      " |-- listing_url: string (nullable = true)\n",
      " |-- living_area: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- location_ids: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- lot_sqft: string (nullable = true)\n",
      " |-- lower_location_id: string (nullable = true)\n",
      " |-- main_level_sqft: string (nullable = true)\n",
      " |-- mls_id: string (nullable = true)\n",
      " |-- mls_name: string (nullable = true)\n",
      " |-- mls_number: string (nullable = true)\n",
      " |-- modified_at: string (nullable = true)\n",
      " |-- num_floors: string (nullable = true)\n",
      " |-- one_quarter_bathrooms: string (nullable = true)\n",
      " |-- open_house_count: string (nullable = true)\n",
      " |-- parking: string (nullable = true)\n",
      " |-- parking_types: string (nullable = true)\n",
      " |-- partial_bathrooms: string (nullable = true)\n",
      " |-- participant: string (nullable = true)\n",
      " |-- patio: string (nullable = true)\n",
      " |-- pool: string (nullable = true)\n",
      " |-- postal: string (nullable = true)\n",
      " |-- price_cents: string (nullable = true)\n",
      " |-- price_cents_sqft: string (nullable = true)\n",
      " |-- property_sub_type: string (nullable = true)\n",
      " |-- property_type: string (nullable = true)\n",
      " |-- provider_category: string (nullable = true)\n",
      " |-- provider_name: string (nullable = true)\n",
      " |-- provider_url: string (nullable = true)\n",
      " |-- province: string (nullable = true)\n",
      " |-- roof: string (nullable = true)\n",
      " |-- roof_types: string (nullable = true)\n",
      " |-- room_count: string (nullable = true)\n",
      " |-- rooms: string (nullable = true)\n",
      " |-- school: string (nullable = true)\n",
      " |-- scoring: string (nullable = true)\n",
      " |-- showing_date: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- stories: string (nullable = true)\n",
      " |-- tags: string (nullable = true)\n",
      " |-- tax_amount: string (nullable = true)\n",
      " |-- three_quarter_bathrooms: string (nullable = true)\n",
      " |-- total_sqft: string (nullable = true)\n",
      " |-- unit_number: string (nullable = true)\n",
      " |-- updated_at: string (nullable = true)\n",
      " |-- utilities: string (nullable = true)\n",
      " |-- water: string (nullable = true)\n",
      " |-- water_heater: string (nullable = true)\n",
      " |-- year_built: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = my_spark.read.format(\"com.mongodb.spark.sql.DefaultSource\")\\\n",
    "    .option(\"database\",\"backend_production\")\\\n",
    "    .option(\"collection\", \"properties\")\\\n",
    "    .schema(df.schema)\\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: string (nullable = true)\n",
      " |-- address_number: string (nullable = true)\n",
      " |-- address_street: string (nullable = true)\n",
      " |-- agent_avatar_url: string (nullable = true)\n",
      " |-- agent_email: string (nullable = true)\n",
      " |-- agent_external_id: string (nullable = true)\n",
      " |-- agent_fname: string (nullable = true)\n",
      " |-- agent_id: string (nullable = true)\n",
      " |-- agent_key: string (nullable = true)\n",
      " |-- agent_lname: string (nullable = true)\n",
      " |-- agent_name: string (nullable = true)\n",
      " |-- agent_phone: string (nullable = true)\n",
      " |-- architecture_style: string (nullable = true)\n",
      " |-- available_showing: string (nullable = true)\n",
      " |-- basement: string (nullable = true)\n",
      " |-- bathrooms: string (nullable = true)\n",
      " |-- beds: string (nullable = true)\n",
      " |-- brokerage: string (nullable = true)\n",
      " |-- brokerage_email: string (nullable = true)\n",
      " |-- brokerage_external_id: string (nullable = true)\n",
      " |-- brokerage_logo_url: string (nullable = true)\n",
      " |-- brokerage_name: string (nullable = true)\n",
      " |-- brokerage_phone: string (nullable = true)\n",
      " |-- brokerage_website_url: string (nullable = true)\n",
      " |-- building_sqft: string (nullable = true)\n",
      " |-- building_type: string (nullable = true)\n",
      " |-- building_utilities: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- cooling_systems: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- cover_public_url: string (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- disclose_address: string (nullable = true)\n",
      " |-- exterior_types: string (nullable = true)\n",
      " |-- external_id: string (nullable = true)\n",
      " |-- external_type: string (nullable = true)\n",
      " |-- external_url: string (nullable = true)\n",
      " |-- fees: string (nullable = true)\n",
      " |-- fireplaces: string (nullable = true)\n",
      " |-- floors: string (nullable = true)\n",
      " |-- floors_number: string (nullable = true)\n",
      " |-- foreclosure: string (nullable = true)\n",
      " |-- full_baths: string (nullable = true)\n",
      " |-- furnished: string (nullable = true)\n",
      " |-- garage: string (nullable = true)\n",
      " |-- geocoded: string (nullable = true)\n",
      " |-- half_baths: string (nullable = true)\n",
      " |-- has_attic: string (nullable = true)\n",
      " |-- has_barbecue_area: string (nullable = true)\n",
      " |-- has_basement: string (nullable = true)\n",
      " |-- has_ceiling_fan: string (nullable = true)\n",
      " |-- has_deck: string (nullable = true)\n",
      " |-- has_disabled_access: string (nullable = true)\n",
      " |-- has_dock: string (nullable = true)\n",
      " |-- has_doorman: string (nullable = true)\n",
      " |-- has_double_pane_windows: string (nullable = true)\n",
      " |-- has_elevator: string (nullable = true)\n",
      " |-- has_fireplace: string (nullable = true)\n",
      " |-- has_garage: string (nullable = true)\n",
      " |-- has_garden: string (nullable = true)\n",
      " |-- has_gated_entry: string (nullable = true)\n",
      " |-- has_green_house: string (nullable = true)\n",
      " |-- has_hot_tub_spa: string (nullable = true)\n",
      " |-- has_jetted_bath_tub: string (nullable = true)\n",
      " |-- has_mother_in_law: string (nullable = true)\n",
      " |-- has_patio: string (nullable = true)\n",
      " |-- has_pond: string (nullable = true)\n",
      " |-- has_pool: string (nullable = true)\n",
      " |-- has_porch: string (nullable = true)\n",
      " |-- has_rv_parking: string (nullable = true)\n",
      " |-- has_sauna: string (nullable = true)\n",
      " |-- has_security_system: string (nullable = true)\n",
      " |-- has_skylight: string (nullable = true)\n",
      " |-- has_sports_court: string (nullable = true)\n",
      " |-- has_sprinkler_system: string (nullable = true)\n",
      " |-- has_vaulted_ceiling: string (nullable = true)\n",
      " |-- has_wet_bar: string (nullable = true)\n",
      " |-- heating_fuels: string (nullable = true)\n",
      " |-- heating_systems: string (nullable = true)\n",
      " |-- images: string (nullable = true)\n",
      " |-- intercom: string (nullable = true)\n",
      " |-- is_cable_ready: string (nullable = true)\n",
      " |-- is_new_construction: string (nullable = true)\n",
      " |-- is_waterfront: string (nullable = true)\n",
      " |-- is_wired: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- lead_routing_email: string (nullable = true)\n",
      " |-- listing_category: string (nullable = true)\n",
      " |-- listing_date: string (nullable = true)\n",
      " |-- listing_status: string (nullable = true)\n",
      " |-- listing_title: string (nullable = true)\n",
      " |-- listing_type: string (nullable = true)\n",
      " |-- listing_url: string (nullable = true)\n",
      " |-- living_area: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- location_ids: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- lot_sqft: string (nullable = true)\n",
      " |-- lower_location_id: string (nullable = true)\n",
      " |-- main_level_sqft: string (nullable = true)\n",
      " |-- mls_id: string (nullable = true)\n",
      " |-- mls_name: string (nullable = true)\n",
      " |-- mls_number: string (nullable = true)\n",
      " |-- modified_at: string (nullable = true)\n",
      " |-- num_floors: string (nullable = true)\n",
      " |-- one_quarter_bathrooms: string (nullable = true)\n",
      " |-- open_house_count: string (nullable = true)\n",
      " |-- parking: string (nullable = true)\n",
      " |-- parking_types: string (nullable = true)\n",
      " |-- partial_bathrooms: string (nullable = true)\n",
      " |-- participant: string (nullable = true)\n",
      " |-- patio: string (nullable = true)\n",
      " |-- pool: string (nullable = true)\n",
      " |-- postal: string (nullable = true)\n",
      " |-- price_cents: string (nullable = true)\n",
      " |-- price_cents_sqft: string (nullable = true)\n",
      " |-- property_sub_type: string (nullable = true)\n",
      " |-- property_type: string (nullable = true)\n",
      " |-- provider_category: string (nullable = true)\n",
      " |-- provider_name: string (nullable = true)\n",
      " |-- provider_url: string (nullable = true)\n",
      " |-- province: string (nullable = true)\n",
      " |-- roof: string (nullable = true)\n",
      " |-- roof_types: string (nullable = true)\n",
      " |-- room_count: string (nullable = true)\n",
      " |-- rooms: string (nullable = true)\n",
      " |-- school: string (nullable = true)\n",
      " |-- scoring: string (nullable = true)\n",
      " |-- showing_date: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- stories: string (nullable = true)\n",
      " |-- tags: string (nullable = true)\n",
      " |-- tax_amount: string (nullable = true)\n",
      " |-- three_quarter_bathrooms: string (nullable = true)\n",
      " |-- total_sqft: string (nullable = true)\n",
      " |-- unit_number: string (nullable = true)\n",
      " |-- updated_at: string (nullable = true)\n",
      " |-- utilities: string (nullable = true)\n",
      " |-- water: string (nullable = true)\n",
      " |-- water_heater: string (nullable = true)\n",
      " |-- year_built: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.registerTempTable(\"table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_ca=sqlContext.sql(\"select * from table where country in ('Canada','CA')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ca=sqlContext.sql(\n",
    "'''\n",
    "select \n",
    "utilities,                \n",
    "parking_types,            \n",
    "year_built,               \n",
    "location_ids,            \n",
    "lower_location_id,        \n",
    "agent_fname,               \n",
    "agent_lname,               \n",
    "property_sub_type,        \n",
    "listing_type,              \n",
    "location,                  \n",
    "postal,                    \n",
    "bathrooms,                 \n",
    "external_type,             \n",
    "agent_key,                 \n",
    "address_street,            \n",
    "beds,                      \n",
    "brokerage_external_id,     \n",
    "brokerage_name,            \n",
    "address_number,            \n",
    "city,                      \n",
    "country,                   \n",
    "external_id,              \n",
    "has_pool,                  \n",
    "geocoded,                  \n",
    "has_fireplace,             \n",
    "has_garage,                \n",
    "images,                    \n",
    "latitude,                  \n",
    "listing_status,            \n",
    "longitude,                 \n",
    "mls_number,                \n",
    "price_cents,               \n",
    "property_type,             \n",
    "province,                  \n",
    "status,                    \n",
    "_id \n",
    "from table where country in ('Canada','CA')\n",
    "'''\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82785"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_ca.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unecessary features (more than 50% records are null)\n",
    "ca_drop_features=[\"has_pond\", #100\n",
    "\"has_dock\",\n",
    "\"foreclosure\",              \n",
    "\"is_wired\",                 \n",
    "\"has_barbecue_area\",        \n",
    "\"is_new_construction\",      \n",
    "\"intercom\",                 \n",
    "\"has_basement\",             \n",
    "\"heating_systems\",          \n",
    "\"heating_fuels\",            \n",
    "\"has_wet_bar\",              \n",
    "\"has_vaulted_ceiling\",      \n",
    "\"has_sprinkler_system\",     \n",
    "\"has_sports_court\",         \n",
    "\"has_skylight\",             \n",
    "\"has_security_system\",      \n",
    "\"has_sauna\",                \n",
    "\"has_rv_parking\",           \n",
    "\"has_porch\",                \n",
    "\"has_ceiling_fan\",         \n",
    "\"has_patio\",                \n",
    "\"has_mother_in_law\",        \n",
    "\"has_jetted_bath_tub\",      \n",
    "\"has_hot_tub_spa\",          \n",
    "\"has_green_house\",          \n",
    "\"has_gated_entry\",          \n",
    "\"has_garden\",               \n",
    "\"has_deck\",                \n",
    "\"has_disabled_access\",      \n",
    "\"has_elevator\",\n",
    "\"cooling_systems\",#            100.00\n",
    "\"brokerage_website_url\",#      100.00\n",
    "\"lead_routing_email\",#         100.00\n",
    "\"architecture_style\",#         100.00\n",
    "\"has_double_pane_windows\",#    100.00\n",
    "\"agent_phone\",#                100.00\n",
    "\"has_doorman\",#                100.00\n",
    "\"brokerage_logo_url\",#         100.00\n",
    "\"fees\",#                       100.00\n",
    "\"external_url\",#               100.00\n",
    "\"agent_id\",#                   100.00\n",
    "\"num_floors\",#                 100.00\n",
    "\"open_houses\",#                100.00\n",
    "\"exterior_types\",#             100.00\n",
    "\"price_cents_sqft\",#           100.00\n",
    "\"brokerage_phone\",#            100.00\n",
    "\"agent_email\",#                100.00\n",
    "\"rooms\",#                      100.00\n",
    "\"tax_amount\",#                 100.00\n",
    "\"tags\",#                       100.00\n",
    "\"agent_avatar_url\",#           100.00\n",
    "\"scoring\",#                    100.00\n",
    "\"school\",#                     100.00\n",
    "\"brokerage_email\",#            100.00\n",
    "\"room_count\",#                 100.00\n",
    "\"building_sqft\",#              99.33\n",
    "\"lot_sqft\",#                   97.40\n",
    "\"total_sqft\",#                 94.25\n",
    "\"roof_types\",#                 86.29\n",
    "\"unit_number\",#81.94\n",
    "\"building_utilities\",#79.65\n",
    "\"floors\",#65.05\n",
    "\"partial_bathrooms\", # 54.05  \n",
    "\"is_waterfront\"                  \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ca.registerTempTable(\"ca_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- utilities: string (nullable = true)\n",
      " |-- parking_types: string (nullable = true)\n",
      " |-- year_built: string (nullable = true)\n",
      " |-- location_ids: string (nullable = true)\n",
      " |-- lower_location_id: string (nullable = true)\n",
      " |-- agent_fname: string (nullable = true)\n",
      " |-- agent_lname: string (nullable = true)\n",
      " |-- property_sub_type: string (nullable = true)\n",
      " |-- listing_type: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- postal: string (nullable = true)\n",
      " |-- bathrooms: string (nullable = true)\n",
      " |-- external_type: string (nullable = true)\n",
      " |-- agent_key: string (nullable = true)\n",
      " |-- address_street: string (nullable = true)\n",
      " |-- beds: string (nullable = true)\n",
      " |-- brokerage_external_id: string (nullable = true)\n",
      " |-- brokerage_name: string (nullable = true)\n",
      " |-- address_number: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- external_id: string (nullable = true)\n",
      " |-- has_pool: string (nullable = true)\n",
      " |-- geocoded: string (nullable = true)\n",
      " |-- has_fireplace: string (nullable = true)\n",
      " |-- has_garage: string (nullable = true)\n",
      " |-- images: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- listing_status: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- mls_number: string (nullable = true)\n",
      " |-- price_cents: string (nullable = true)\n",
      " |-- property_type: string (nullable = true)\n",
      " |-- province: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- _id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ca.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25018"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_ca.select(\"utilities\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|           utilities|count(1)|\n",
      "+--------------------+--------+\n",
      "|                null|   22909|\n",
      "|             [\"All\"]|    3885|\n",
      "|[\"Washer\", \"Dryer...|    2713|\n",
      "|[\"Refrigerator\", ...|     993|\n",
      "|[\"Dishwasher\", \"R...|     835|\n",
      "|  [\"Central Vacuum\"]|     627|\n",
      "|            [\"None\"]|     495|\n",
      "|      [\"Dishwasher\"]|     474|\n",
      "|[\"Washer\", \"Refri...|     414|\n",
      "|[\"All\", \"Central ...|     391|\n",
      "|     [\"See remarks\"]|     387|\n",
      "|[\"Dryer - Electri...|     347|\n",
      "|[\"Dryer - Electri...|     338|\n",
      "|[\"Garage door ope...|     320|\n",
      "|[\"Dishwasher\", \"D...|     281|\n",
      "|[\"Dryer\", \"Dishwa...|     261|\n",
      "|[\"Washer\", \"Refri...|     252|\n",
      "|[\"Garage door ope...|     236|\n",
      "|[\"Refrigerator\", ...|     225|\n",
      "|[\"Washer\", \"Dishw...|     206|\n",
      "+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select utilities, count(*) from ca_table group by 1 order by count(*) desc \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.sql(\"select utilities, count(*) from ca_table group by 1 order by count(*) desc \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|instr(utilities, Washer)|\n",
      "+------------------------+\n",
      "|                    null|\n",
      "|                    null|\n",
      "|                      45|\n",
      "|                       0|\n",
      "|                    null|\n",
      "|                      21|\n",
      "|                      67|\n",
      "|                      35|\n",
      "|                      57|\n",
      "|                    null|\n",
      "|                       0|\n",
      "|                       0|\n",
      "|                    null|\n",
      "|                       0|\n",
      "|                      35|\n",
      "|                    null|\n",
      "|                    null|\n",
      "|                      21|\n",
      "|                    null|\n",
      "|                    null|\n",
      "+------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select INSTR(utilities,'Washer') as water,  select INSTR(utilities,'Dryer') as Dryer from ca_table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ca_drop=df_ca.drop(ca_drop_features).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=new_df.filter(new_df[\"country\"]==\"CA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18343"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "new_df.filter(F.isnull(\"state\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.where(col(k).isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2369118"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.where(new_df.state.isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Column' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-145-ae6705b97981>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"state\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'Column' object is not callable"
     ]
    }
   ],
   "source": [
    "test.state.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of null rows in each column - very slow\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "aa=test.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in test.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+--------------+----------------+-----------+-----------------+-----------+--------+---------+-----------+----------+-----------+------------------+-----------------+--------+---------+----+---------+---------------+---------------------+------------------+--------------+---------------+---------------------+-------------+-------------+------------------+----+---------------+-------+----------------+----------+-----------+----------------+--------------+-----------+-------------+------------+-----+----------+------+-------------+-----------+----------+---------+------+--------+----------+---------+-----------------+------------+---------------+--------+-------------------+--------+-----------+-----------------------+------------+-------------+----------+----------+---------------+---------------+---------------+-------------------+-----------------+---------+--------+--------+---------+--------------+---------+-------------------+------------+----------------+--------------------+-------------------+-----------+-------------+---------------+------+--------+--------------+-------------------+-------------+--------+--------+------------------+----------------+------------+--------------+-------------+------------+-----------+-----------+--------+------------+---------+--------+-----------------+---------------+------+--------+----------+-----------+----------+---------------------+----------------+-------+-------------+-----------------+-----------+-----+-----+------+-----------+----------------+-----------------+-------------+-----------------+-------------+------------+--------+-----+----------+----------+-----+------+-------+------------+-----+------+-------+-----+----------+-----------------------+----------+-----------+----------+---------+-----+------------+----------+\n",
      "|_id|address_number|address_street|agent_avatar_url|agent_email|agent_external_id|agent_fname|agent_id|agent_key|agent_lname|agent_name|agent_phone|architecture_style|available_showing|basement|bathrooms|beds|brokerage|brokerage_email|brokerage_external_id|brokerage_logo_url|brokerage_name|brokerage_phone|brokerage_website_url|building_sqft|building_type|building_utilities|city|cooling_systems|country|cover_public_url|created_at|description|disclose_address|exterior_types|external_id|external_type|external_url| fees|fireplaces|floors|floors_number|foreclosure|full_baths|furnished|garage|geocoded|half_baths|has_attic|has_barbecue_area|has_basement|has_ceiling_fan|has_deck|has_disabled_access|has_dock|has_doorman|has_double_pane_windows|has_elevator|has_fireplace|has_garage|has_garden|has_gated_entry|has_green_house|has_hot_tub_spa|has_jetted_bath_tub|has_mother_in_law|has_patio|has_pond|has_pool|has_porch|has_rv_parking|has_sauna|has_security_system|has_skylight|has_sports_court|has_sprinkler_system|has_vaulted_ceiling|has_wet_bar|heating_fuels|heating_systems|images|intercom|is_cable_ready|is_new_construction|is_waterfront|is_wired|latitude|lead_routing_email|listing_category|listing_date|listing_status|listing_title|listing_type|listing_url|living_area|location|location_ids|longitude|lot_sqft|lower_location_id|main_level_sqft|mls_id|mls_name|mls_number|modified_at|num_floors|one_quarter_bathrooms|open_house_count|parking|parking_types|partial_bathrooms|participant|patio| pool|postal|price_cents|price_cents_sqft|property_sub_type|property_type|provider_category|provider_name|provider_url|province| roof|roof_types|room_count|rooms|school|scoring|showing_date|state|status|stories| tags|tax_amount|three_quarter_bathrooms|total_sqft|unit_number|updated_at|utilities|water|water_heater|year_built|\n",
      "+---+--------------+--------------+----------------+-----------+-----------------+-----------+--------+---------+-----------+----------+-----------+------------------+-----------------+--------+---------+----+---------+---------------+---------------------+------------------+--------------+---------------+---------------------+-------------+-------------+------------------+----+---------------+-------+----------------+----------+-----------+----------------+--------------+-----------+-------------+------------+-----+----------+------+-------------+-----------+----------+---------+------+--------+----------+---------+-----------------+------------+---------------+--------+-------------------+--------+-----------+-----------------------+------------+-------------+----------+----------+---------------+---------------+---------------+-------------------+-----------------+---------+--------+--------+---------+--------------+---------+-------------------+------------+----------------+--------------------+-------------------+-----------+-------------+---------------+------+--------+--------------+-------------------+-------------+--------+--------+------------------+----------------+------------+--------------+-------------+------------+-----------+-----------+--------+------------+---------+--------+-----------------+---------------+------+--------+----------+-----------+----------+---------------------+----------------+-------+-------------+-----------------+-----------+-----+-----+------+-----------+----------------+-----------------+-------------+-----------------+-------------+------------+--------+-----+----------+----------+-----+------+-------+------------+-----+------+-------+-----+----------+-----------------------+----------+-----------+----------+---------+-----+------------+----------+\n",
      "|  0|             0|             0|           18343|      17619|            18343|      17619|   17619|    17619|      17619|     18343|      17619|             17619|            18343|   18343|        0|   0|    18343|          17619|                18343|             18286|             0|          17619|                18282|        17986|        18343|               724|   0|          18265|      0|              74|         0|         76|           18343|         18342|          0|            0|       18343|18343|     18343| 18339|        18343|      18342|     18343|    18343| 18343|       0|     18343|    18343|            18343|         724|          18341|   18309|              18342|   18343|      18343|                  18343|       18343|        18334|      4397|     18341|          18343|          18343|          18341|              18343|            18341|    18336|   18343|    4381|    18342|         18343|    18343|              18343|       18343|           18343|               18340|              18340|      18343|        18076|          18059|     0|   18343|         18343|              17619|        18343|   18342|       0|             17619|           18343|       17619|             0|        18343|         370|      17619|      18343|    1635|        5502|        0|    5156|             5502|          18343| 18343|   18343|         0|          2|     17956|                18343|           18343|  18343|        18312|            17619|      18343|18343|18343|     0|         21|           17993|            17619|        17619|            18343|        18343|       18343|       0|18343|     17619|     18343|17734| 18343|  18343|       18343|18343|     0|  18343|18343|       724|                  18343|     18343|      18244|         0|     1902|18343|       18343|       503|\n",
      "+---+--------------+--------------+----------------+-----------+-----------------+-----------+--------+---------+-----------+----------+-----------+------------------+-----------------+--------+---------+----+---------+---------------+---------------------+------------------+--------------+---------------+---------------------+-------------+-------------+------------------+----+---------------+-------+----------------+----------+-----------+----------------+--------------+-----------+-------------+------------+-----+----------+------+-------------+-----------+----------+---------+------+--------+----------+---------+-----------------+------------+---------------+--------+-------------------+--------+-----------+-----------------------+------------+-------------+----------+----------+---------------+---------------+---------------+-------------------+-----------------+---------+--------+--------+---------+--------------+---------+-------------------+------------+----------------+--------------------+-------------------+-----------+-------------+---------------+------+--------+--------------+-------------------+-------------+--------+--------+------------------+----------------+------------+--------------+-------------+------------+-----------+-----------+--------+------------+---------+--------+-----------------+---------------+------+--------+----------+-----------+----------+---------------------+----------------+-------+-------------+-----------------+-----------+-----+-----+------+-----------+----------------+-----------------+-------------+-----------------+-------------+------------+--------+-----+----------+----------+-----+------+-------+------------+-----+------+-------+-----+----------+-----------------------+----------+-----------+----------+---------+-----+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aa.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o8754.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Total size of serialized results of 39 tasks (1037.4 MB) is bigger than spark.driver.maxResultSize (1024.0 MB)\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:299)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3257)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3254)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3254)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-122-a009d0e1b8bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpandas_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mtoPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2141\u001b[0m         \u001b[0;31m# Below is toPandas without Arrow optimization.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2142\u001b[0;31m         \u001b[0mpdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2144\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    531\u001b[0m         \"\"\"\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o8754.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Total size of serialized results of 39 tasks (1037.4 MB) is bigger than spark.driver.maxResultSize (1024.0 MB)\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:299)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3257)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3254)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3254)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "pandas_df=new_df.toPandas()\n",
    "#df.filter(df(colName).isNull || df(colName) === \"\" || df(colName).isNaN).count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "df_agg = df.agg(*[F.count(F.when(F.isnull(c), c)).alias(c) for c in new_df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'info'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-121-b883df5fcbe1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnew_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1298\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m             raise AttributeError(\n\u001b[0;32m-> 1300\u001b[0;31m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0m\u001b[1;32m   1301\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'info'"
     ]
    }
   ],
   "source": [
    "new_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o9476.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 8.0 failed 1 times, most recent failure: Lost task 0.0 in stage 8.0 (TID 8, localhost, executor driver): com.mongodb.spark.exceptions.MongoTypeConversionException: Cannot cast BOOLEAN into a NullType (value: BsonBoolean{value=true})\n\tat com.mongodb.spark.sql.MapFunctions$.com$mongodb$spark$sql$MapFunctions$$convertToDataType(MapFunctions.scala:200)\n\tat com.mongodb.spark.sql.MapFunctions$$anonfun$3.apply(MapFunctions.scala:39)\n\tat com.mongodb.spark.sql.MapFunctions$$anonfun$3.apply(MapFunctions.scala:37)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)\n\tat com.mongodb.spark.sql.MapFunctions$.documentToRow(MapFunctions.scala:37)\n\tat com.mongodb.spark.sql.MongoRelation$$anonfun$buildScan$1.apply(MongoRelation.scala:58)\n\tat com.mongodb.spark.sql.MongoRelation$$anonfun$buildScan$1.apply(MongoRelation.scala:58)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:186)\n\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.<init>(TungstenAggregationIterator.scala:360)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec$$anonfun$doExecute$1$$anonfun$4.apply(HashAggregateExec.scala:112)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec$$anonfun$doExecute$1$$anonfun$4.apply(HashAggregateExec.scala:102)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:853)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:853)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3383)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2758)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.mongodb.spark.exceptions.MongoTypeConversionException: Cannot cast BOOLEAN into a NullType (value: BsonBoolean{value=true})\n\tat com.mongodb.spark.sql.MapFunctions$.com$mongodb$spark$sql$MapFunctions$$convertToDataType(MapFunctions.scala:200)\n\tat com.mongodb.spark.sql.MapFunctions$$anonfun$3.apply(MapFunctions.scala:39)\n\tat com.mongodb.spark.sql.MapFunctions$$anonfun$3.apply(MapFunctions.scala:37)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)\n\tat com.mongodb.spark.sql.MapFunctions$.documentToRow(MapFunctions.scala:37)\n\tat com.mongodb.spark.sql.MongoRelation$$anonfun$buildScan$1.apply(MongoRelation.scala:58)\n\tat com.mongodb.spark.sql.MongoRelation$$anonfun$buildScan$1.apply(MongoRelation.scala:58)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:186)\n\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.<init>(TungstenAggregationIterator.scala:360)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec$$anonfun$doExecute$1$$anonfun$4.apply(HashAggregateExec.scala:112)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec$$anonfun$doExecute$1$$anonfun$4.apply(HashAggregateExec.scala:102)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:853)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:853)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-118-5f204641caec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_agg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \"\"\"\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o9476.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 8.0 failed 1 times, most recent failure: Lost task 0.0 in stage 8.0 (TID 8, localhost, executor driver): com.mongodb.spark.exceptions.MongoTypeConversionException: Cannot cast BOOLEAN into a NullType (value: BsonBoolean{value=true})\n\tat com.mongodb.spark.sql.MapFunctions$.com$mongodb$spark$sql$MapFunctions$$convertToDataType(MapFunctions.scala:200)\n\tat com.mongodb.spark.sql.MapFunctions$$anonfun$3.apply(MapFunctions.scala:39)\n\tat com.mongodb.spark.sql.MapFunctions$$anonfun$3.apply(MapFunctions.scala:37)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)\n\tat com.mongodb.spark.sql.MapFunctions$.documentToRow(MapFunctions.scala:37)\n\tat com.mongodb.spark.sql.MongoRelation$$anonfun$buildScan$1.apply(MongoRelation.scala:58)\n\tat com.mongodb.spark.sql.MongoRelation$$anonfun$buildScan$1.apply(MongoRelation.scala:58)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:186)\n\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.<init>(TungstenAggregationIterator.scala:360)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec$$anonfun$doExecute$1$$anonfun$4.apply(HashAggregateExec.scala:112)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec$$anonfun$doExecute$1$$anonfun$4.apply(HashAggregateExec.scala:102)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:853)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:853)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3383)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2758)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.mongodb.spark.exceptions.MongoTypeConversionException: Cannot cast BOOLEAN into a NullType (value: BsonBoolean{value=true})\n\tat com.mongodb.spark.sql.MapFunctions$.com$mongodb$spark$sql$MapFunctions$$convertToDataType(MapFunctions.scala:200)\n\tat com.mongodb.spark.sql.MapFunctions$$anonfun$3.apply(MapFunctions.scala:39)\n\tat com.mongodb.spark.sql.MapFunctions$$anonfun$3.apply(MapFunctions.scala:37)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)\n\tat com.mongodb.spark.sql.MapFunctions$.documentToRow(MapFunctions.scala:37)\n\tat com.mongodb.spark.sql.MongoRelation$$anonfun$buildScan$1.apply(MongoRelation.scala:58)\n\tat com.mongodb.spark.sql.MongoRelation$$anonfun$buildScan$1.apply(MongoRelation.scala:58)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:186)\n\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.<init>(TungstenAggregationIterator.scala:360)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec$$anonfun$doExecute$1$$anonfun$4.apply(HashAggregateExec.scala:112)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec$$anonfun$doExecute$1$$anonfun$4.apply(HashAggregateExec.scala:102)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:853)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:853)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "df_agg.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df= my_spark.read.format(\"com.mongodb.spark.sql.DefaultSource\")\\\n",
    "    .option(\"database\",\"backend_production\")\\\n",
    "    .option(\"collection\", \"properties\")\\\n",
    "    .option(\"inferSchema\", None)\\\n",
    "    .load()\n",
    ".option(\"inferSchema\", None)\\\n",
    "    .option(schema,data_df.schema)\\\n",
    "  '''  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.coalesce(1).write.format('json').save('/your_path/output_directory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GroupedData' object has no attribute 'show'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-170-dfb711e1e2af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnew_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"country\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'GroupedData' object has no attribute 'show'"
     ]
    }
   ],
   "source": [
    "new_df.groupby(\"country\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.registerTempTable(\"table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ca=sqlContext.sql(\"select * from table where country = ('CA')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ca.coalesce(1).write.format('json').save('df_ca.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|country|count(1)|\n",
      "+-------+--------+\n",
      "|     MX|       1|\n",
      "|   null|   16289|\n",
      "|     CA|   18343|\n",
      "|     IO|       1|\n",
      "|     US| 2269170|\n",
      "| Mexico|       8|\n",
      "| Canada|   64442|\n",
      "|     PR|     891|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select country, count(*) from table group by country\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|country|\n",
      "+-------+\n",
      "|     MX|\n",
      "|   null|\n",
      "|     CA|\n",
      "|     IO|\n",
      "|     US|\n",
      "| Mexico|\n",
      "| Canada|\n",
      "|     PR|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.select(\"country\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+-------+--------+\n",
      "|        city|state|country|count(1)|\n",
      "+------------+-----+-------+--------+\n",
      "|Indianapolis| null|   null|     834|\n",
      "|      Boston| null|   null|     752|\n",
      "|    Plymouth| null|   null|     257|\n",
      "|  Evansville| null|   null|     221|\n",
      "|      Muncie| null|   null|     201|\n",
      "|  Barnstable| null|   null|     188|\n",
      "|  Fort Wayne| null|   null|     176|\n",
      "| Springfield| null|   null|     170|\n",
      "|      Kokomo| null|   null|     168|\n",
      "|      Marion| null|   null|     166|\n",
      "|      Carmel| null|   null|     164|\n",
      "|    Falmouth| null|   null|     148|\n",
      "|   Westfield| null|   null|     145|\n",
      "|     Fishers| null|   null|     141|\n",
      "| New Bedford| null|   null|     140|\n",
      "|    Anderson| null|   null|     132|\n",
      "|      Newton| null|   null|     130|\n",
      "|  Fall River| null|   null|     125|\n",
      "|    Yarmouth| null|   null|     118|\n",
      "|     Bedford| null|   null|     110|\n",
      "+------------+-----+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select city,state, country, count(*) from table where country is null group by city, state, country order by count(*) desc\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- address_number: string (nullable = true)\n",
      " |-- address_street: string (nullable = true)\n",
      " |-- agent_avatar_url: null (nullable = true)\n",
      " |-- agent_email: string (nullable = true)\n",
      " |-- agent_external_id: string (nullable = true)\n",
      " |-- agent_fname: string (nullable = true)\n",
      " |-- agent_id: string (nullable = true)\n",
      " |-- agent_key: string (nullable = true)\n",
      " |-- agent_lname: string (nullable = true)\n",
      " |-- agent_name: string (nullable = true)\n",
      " |-- agent_phone: string (nullable = true)\n",
      " |-- architecture_style: string (nullable = true)\n",
      " |-- available_showing: boolean (nullable = true)\n",
      " |-- basement: boolean (nullable = true)\n",
      " |-- bathrooms: double (nullable = true)\n",
      " |-- beds: integer (nullable = true)\n",
      " |-- brokerage: struct (nullable = true)\n",
      " |    |-- address: struct (nullable = true)\n",
      " |    |    |-- commons:City: string (nullable = true)\n",
      " |    |    |-- commons:Country: string (nullable = true)\n",
      " |    |    |-- commons:FullStreetAddress: string (nullable = true)\n",
      " |    |    |-- commons:PostalCode: string (nullable = true)\n",
      " |    |    |-- commons:StateOrProvince: string (nullable = true)\n",
      " |    |    |-- commons:UnitNumber: string (nullable = true)\n",
      " |    |    |-- commons:address-preference-order: string (nullable = true)\n",
      " |    |    |-- commons:preference-order: string (nullable = true)\n",
      " |    |-- logo_url: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- phone: string (nullable = true)\n",
      " |    |-- website_url: string (nullable = true)\n",
      " |-- brokerage_email: string (nullable = true)\n",
      " |-- brokerage_external_id: string (nullable = true)\n",
      " |-- brokerage_logo_url: string (nullable = true)\n",
      " |-- brokerage_name: string (nullable = true)\n",
      " |-- brokerage_phone: string (nullable = true)\n",
      " |-- brokerage_website_url: string (nullable = true)\n",
      " |-- building_sqft: double (nullable = true)\n",
      " |-- building_type: string (nullable = true)\n",
      " |-- building_utilities: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- cooling_systems: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- cover_public_url: string (nullable = true)\n",
      " |-- created_at: timestamp (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- disclose_address: string (nullable = true)\n",
      " |-- exterior_types: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- external_id: string (nullable = true)\n",
      " |-- external_type: struct (nullable = true)\n",
      " |    |-- symbol: string (nullable = true)\n",
      " |-- external_url: string (nullable = true)\n",
      " |-- fees: null (nullable = true)\n",
      " |-- fireplaces: string (nullable = true)\n",
      " |-- floors: string (nullable = true)\n",
      " |-- floors_number: double (nullable = true)\n",
      " |-- foreclosure: boolean (nullable = true)\n",
      " |-- full_baths: integer (nullable = true)\n",
      " |-- furnished: null (nullable = true)\n",
      " |-- garage: boolean (nullable = true)\n",
      " |-- geocoded: boolean (nullable = true)\n",
      " |-- half_baths: integer (nullable = true)\n",
      " |-- has_attic: boolean (nullable = true)\n",
      " |-- has_barbecue_area: boolean (nullable = true)\n",
      " |-- has_basement: boolean (nullable = true)\n",
      " |-- has_ceiling_fan: boolean (nullable = true)\n",
      " |-- has_deck: boolean (nullable = true)\n",
      " |-- has_disabled_access: boolean (nullable = true)\n",
      " |-- has_dock: boolean (nullable = true)\n",
      " |-- has_doorman: boolean (nullable = true)\n",
      " |-- has_double_pane_windows: boolean (nullable = true)\n",
      " |-- has_elevator: boolean (nullable = true)\n",
      " |-- has_fireplace: boolean (nullable = true)\n",
      " |-- has_garage: boolean (nullable = true)\n",
      " |-- has_garden: boolean (nullable = true)\n",
      " |-- has_gated_entry: boolean (nullable = true)\n",
      " |-- has_green_house: boolean (nullable = true)\n",
      " |-- has_hot_tub_spa: boolean (nullable = true)\n",
      " |-- has_jetted_bath_tub: boolean (nullable = true)\n",
      " |-- has_mother_in_law: boolean (nullable = true)\n",
      " |-- has_patio: boolean (nullable = true)\n",
      " |-- has_pond: boolean (nullable = true)\n",
      " |-- has_pool: boolean (nullable = true)\n",
      " |-- has_porch: boolean (nullable = true)\n",
      " |-- has_rv_parking: boolean (nullable = true)\n",
      " |-- has_sauna: boolean (nullable = true)\n",
      " |-- has_security_system: boolean (nullable = true)\n",
      " |-- has_skylight: boolean (nullable = true)\n",
      " |-- has_sports_court: boolean (nullable = true)\n",
      " |-- has_sprinkler_system: boolean (nullable = true)\n",
      " |-- has_vaulted_ceiling: boolean (nullable = true)\n",
      " |-- has_wet_bar: boolean (nullable = true)\n",
      " |-- heating_fuels: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- heating_systems: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- images: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- _id: struct (nullable = true)\n",
      " |    |    |    |-- oid: string (nullable = true)\n",
      " |    |    |-- created_at: timestamp (nullable = true)\n",
      " |    |    |-- modified_at: timestamp (nullable = true)\n",
      " |    |    |-- original_url: string (nullable = true)\n",
      " |    |    |-- sha1: string (nullable = true)\n",
      " |    |    |-- updated_at: timestamp (nullable = true)\n",
      " |    |    |-- url: string (nullable = true)\n",
      " |-- intercom: boolean (nullable = true)\n",
      " |-- is_cable_ready: boolean (nullable = true)\n",
      " |-- is_new_construction: boolean (nullable = true)\n",
      " |-- is_waterfront: boolean (nullable = true)\n",
      " |-- is_wired: boolean (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- lead_routing_email: string (nullable = true)\n",
      " |-- listing_category: string (nullable = true)\n",
      " |-- listing_date: timestamp (nullable = true)\n",
      " |-- listing_status: struct (nullable = true)\n",
      " |    |-- symbol: string (nullable = true)\n",
      " |-- listing_title: string (nullable = true)\n",
      " |-- listing_type: string (nullable = true)\n",
      " |-- listing_url: string (nullable = true)\n",
      " |-- living_area: string (nullable = true)\n",
      " |-- location: struct (nullable = true)\n",
      " |    |-- type: string (nullable = true)\n",
      " |    |-- coordinates: array (nullable = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |-- location_ids: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- oid: string (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- lot_sqft: double (nullable = true)\n",
      " |-- lower_location_id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- main_level_sqft: double (nullable = true)\n",
      " |-- mls_id: string (nullable = true)\n",
      " |-- mls_name: string (nullable = true)\n",
      " |-- mls_number: string (nullable = true)\n",
      " |-- modified_at: timestamp (nullable = true)\n",
      " |-- num_floors: double (nullable = true)\n",
      " |-- one_quarter_bathrooms: integer (nullable = true)\n",
      " |-- open_house_count: integer (nullable = true)\n",
      " |-- parking: string (nullable = true)\n",
      " |-- parking_types: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- partial_bathrooms: integer (nullable = true)\n",
      " |-- participant: struct (nullable = true)\n",
      " |    |-- email: string (nullable = true)\n",
      " |    |-- first_name: string (nullable = true)\n",
      " |    |-- id: string (nullable = true)\n",
      " |    |-- key: string (nullable = true)\n",
      " |    |-- last_name: string (nullable = true)\n",
      " |    |-- office_phone: string (nullable = true)\n",
      " |    |-- primary_contact_phone: string (nullable = true)\n",
      " |    |-- website_url: string (nullable = true)\n",
      " |-- patio: boolean (nullable = true)\n",
      " |-- pool: null (nullable = true)\n",
      " |-- postal: string (nullable = true)\n",
      " |-- price_cents: integer (nullable = true)\n",
      " |-- price_cents_sqft: integer (nullable = true)\n",
      " |-- property_sub_type: string (nullable = true)\n",
      " |-- property_type: string (nullable = true)\n",
      " |-- provider_category: string (nullable = true)\n",
      " |-- provider_name: string (nullable = true)\n",
      " |-- provider_url: string (nullable = true)\n",
      " |-- province: string (nullable = true)\n",
      " |-- roof: string (nullable = true)\n",
      " |-- roof_types: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- room_count: integer (nullable = true)\n",
      " |-- rooms: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- school: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- scoring: null (nullable = true)\n",
      " |-- showing_date: timestamp (nullable = true)\n",
      " |-- state: null (nullable = true)\n",
      " |-- status: struct (nullable = true)\n",
      " |    |-- symbol: string (nullable = true)\n",
      " |-- stories: null (nullable = true)\n",
      " |-- tags: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- symbol: string (nullable = true)\n",
      " |-- tax_amount: string (nullable = true)\n",
      " |-- three_quarter_bathrooms: integer (nullable = true)\n",
      " |-- total_sqft: double (nullable = true)\n",
      " |-- unit_number: string (nullable = true)\n",
      " |-- updated_at: timestamp (nullable = true)\n",
      " |-- utilities: string (nullable = true)\n",
      " |-- water: string (nullable = true)\n",
      " |-- water_heater: string (nullable = true)\n",
      " |-- year_built: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Following are the Schema type mismatches when Spark loads property data\n",
    "\n",
    "'total_sqft' - Cannot cast STRING into a DoubleType (value: BsonString{value='1398'})\n",
    "'building_sqft' - Cannot cast STRING into a DoubleType (value: BsonString{value=''})\n",
    "'lot_sqft' - Cannot cast STRING into a DoubleType (value: BsonString{value='NA'})\n",
    "'external_url'- Cannot cast STRING into a DoubleType (value: BsonString{value='1398'})\n",
    "'furnished' - Cannot cast BOOLEAN into a NullType (value: BsonBoolean{value=false})\n",
    "'main_level_sqft'  - Cannot cast STRING into a DoubleType (value: BsonString{value='1398'})\n",
    "'open_house_count' - Cannot cast STRING into a NullType (value: BsonString{value=''})    \n",
    "'state' - Cannot cast STRING into a NullType (value: BsonString{value='active'})\n",
    "'stories' - Cannot cast STRING into a IntegerType (value: BsonString{value=''})\n",
    "'total_sqft'- Cannot cast STRING into a DoubleType (value: BsonString{value='1398'})\n",
    "'available_showing' -  Cannot cast BOOLEAN into a NullType \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "data_df = df.withColumn(\"state\", df[\"state\"].cast(StringType()))\n",
    "data_df1 = data_df.withColumn(\"stories\", df[\"stories\"].cast(StringType()))\n",
    "data_df = data_df1.withColumn(\"total_sqft\", df[\"total_sqft\"].cast(StringType()))\n",
    "data_df = data_df.withColumn(\"building_sqft\", df[\"building_sqft\"].cast(StringType()))\n",
    "data_df = data_df.withColumn(\"lot_sqft\", df[\"lot_sqft\"].cast(StringType()))\n",
    "data_df = data_df.withColumn(\"external_url\", df[\"external_url\"].cast(StringType()))\n",
    "data_df = data_df.withColumn(\"furnished\", df[\"furnished\"].cast(StringType()))\n",
    "data_df = data_df.withColumn(\"main_level_sqft\", df[\"main_level_sqft\"].cast(StringType()))\n",
    "data_df = data_df.withColumn(\"open_house_count\", df[\"open_house_count\"].cast(StringType()))\n",
    "data_df = data_df.withColumn(\"total_sqft\", df[\"total_sqft\"].cast(StringType()))\n",
    "data_df = data_df.withColumn(\"available_showing\", df[\"available_showing\"].cast(StringType()))\n",
    "data_df = data_df.withColumn(\"agent_avatar_url\", df[\"agent_avatar_url\"].cast(StringType()))\n",
    "data_df = data_df.withColumn(\"basement\", df[\"basement\"].cast(StringType()))\n",
    "data_df = data_df.withColumn(\"bathrooms\", df[\"bathrooms\"].cast(StringType()))\n",
    "data_df = data_df.withColumn(\"floors_number\", df[\"floors_number\"].cast(StringType()))\n",
    "data_df = data_df.withColumn(\"foreclosure\", df[\"foreclosure\"].cast(StringType()))\n",
    "data_df = data_df.withColumn(\"garage\", df[\"garage\"].cast(StringType()))\n",
    "data_df = data_df.withColumn(\"geocoded\", df[\"geocoded\"].cast(StringType()))\n",
    "data_df = data_df.withColumn(\"num_floors\", df[\"num_floors\"].cast(StringType()))\n",
    "data_df = data_df.withColumn(\"open_house_count\", df[\"open_house_count\"].cast(StringType()))\n",
    "data_df = data_df.withColumn(\"scoring\", df[\"scoring\"].cast(StringType()))\n",
    "data_df = data_df.withColumn(\"showing_date\", df[\"showing_date\"].cast(StringType()))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df.columns:\n",
    "    df= df.withColumn(i, df[i].cast(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: string (nullable = true)\n",
      " |-- address_number: string (nullable = true)\n",
      " |-- address_street: string (nullable = true)\n",
      " |-- agent_avatar_url: string (nullable = true)\n",
      " |-- agent_email: string (nullable = true)\n",
      " |-- agent_external_id: string (nullable = true)\n",
      " |-- agent_fname: string (nullable = true)\n",
      " |-- agent_id: string (nullable = true)\n",
      " |-- agent_key: string (nullable = true)\n",
      " |-- agent_lname: string (nullable = true)\n",
      " |-- agent_name: string (nullable = true)\n",
      " |-- agent_phone: string (nullable = true)\n",
      " |-- architecture_style: string (nullable = true)\n",
      " |-- available_showing: string (nullable = true)\n",
      " |-- basement: string (nullable = true)\n",
      " |-- bathrooms: string (nullable = true)\n",
      " |-- beds: string (nullable = true)\n",
      " |-- brokerage: string (nullable = true)\n",
      " |-- brokerage_email: string (nullable = true)\n",
      " |-- brokerage_external_id: string (nullable = true)\n",
      " |-- brokerage_logo_url: string (nullable = true)\n",
      " |-- brokerage_name: string (nullable = true)\n",
      " |-- brokerage_phone: string (nullable = true)\n",
      " |-- brokerage_website_url: string (nullable = true)\n",
      " |-- building_sqft: string (nullable = true)\n",
      " |-- building_type: string (nullable = true)\n",
      " |-- building_utilities: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- cooling_systems: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- cover_public_url: string (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- disclose_address: string (nullable = true)\n",
      " |-- exterior_types: string (nullable = true)\n",
      " |-- external_id: string (nullable = true)\n",
      " |-- external_type: string (nullable = true)\n",
      " |-- external_url: string (nullable = true)\n",
      " |-- fees: string (nullable = true)\n",
      " |-- fireplaces: string (nullable = true)\n",
      " |-- floors: string (nullable = true)\n",
      " |-- floors_number: string (nullable = true)\n",
      " |-- foreclosure: string (nullable = true)\n",
      " |-- full_baths: string (nullable = true)\n",
      " |-- furnished: string (nullable = true)\n",
      " |-- garage: string (nullable = true)\n",
      " |-- geocoded: string (nullable = true)\n",
      " |-- half_baths: string (nullable = true)\n",
      " |-- has_attic: string (nullable = true)\n",
      " |-- has_barbecue_area: string (nullable = true)\n",
      " |-- has_basement: string (nullable = true)\n",
      " |-- has_ceiling_fan: string (nullable = true)\n",
      " |-- has_deck: string (nullable = true)\n",
      " |-- has_disabled_access: string (nullable = true)\n",
      " |-- has_dock: string (nullable = true)\n",
      " |-- has_doorman: string (nullable = true)\n",
      " |-- has_double_pane_windows: string (nullable = true)\n",
      " |-- has_elevator: string (nullable = true)\n",
      " |-- has_fireplace: string (nullable = true)\n",
      " |-- has_garage: string (nullable = true)\n",
      " |-- has_garden: string (nullable = true)\n",
      " |-- has_gated_entry: string (nullable = true)\n",
      " |-- has_green_house: string (nullable = true)\n",
      " |-- has_hot_tub_spa: string (nullable = true)\n",
      " |-- has_jetted_bath_tub: string (nullable = true)\n",
      " |-- has_mother_in_law: string (nullable = true)\n",
      " |-- has_patio: string (nullable = true)\n",
      " |-- has_pond: string (nullable = true)\n",
      " |-- has_pool: string (nullable = true)\n",
      " |-- has_porch: string (nullable = true)\n",
      " |-- has_rv_parking: string (nullable = true)\n",
      " |-- has_sauna: string (nullable = true)\n",
      " |-- has_security_system: string (nullable = true)\n",
      " |-- has_skylight: string (nullable = true)\n",
      " |-- has_sports_court: string (nullable = true)\n",
      " |-- has_sprinkler_system: string (nullable = true)\n",
      " |-- has_vaulted_ceiling: string (nullable = true)\n",
      " |-- has_wet_bar: string (nullable = true)\n",
      " |-- heating_fuels: string (nullable = true)\n",
      " |-- heating_systems: string (nullable = true)\n",
      " |-- images: string (nullable = true)\n",
      " |-- intercom: string (nullable = true)\n",
      " |-- is_cable_ready: string (nullable = true)\n",
      " |-- is_new_construction: string (nullable = true)\n",
      " |-- is_waterfront: string (nullable = true)\n",
      " |-- is_wired: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- lead_routing_email: string (nullable = true)\n",
      " |-- listing_category: string (nullable = true)\n",
      " |-- listing_date: string (nullable = true)\n",
      " |-- listing_status: string (nullable = true)\n",
      " |-- listing_title: string (nullable = true)\n",
      " |-- listing_type: string (nullable = true)\n",
      " |-- listing_url: string (nullable = true)\n",
      " |-- living_area: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- location_ids: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- lot_sqft: string (nullable = true)\n",
      " |-- lower_location_id: string (nullable = true)\n",
      " |-- main_level_sqft: string (nullable = true)\n",
      " |-- mls_id: string (nullable = true)\n",
      " |-- mls_name: string (nullable = true)\n",
      " |-- mls_number: string (nullable = true)\n",
      " |-- modified_at: string (nullable = true)\n",
      " |-- num_floors: string (nullable = true)\n",
      " |-- one_quarter_bathrooms: string (nullable = true)\n",
      " |-- open_house_count: string (nullable = true)\n",
      " |-- parking: string (nullable = true)\n",
      " |-- parking_types: string (nullable = true)\n",
      " |-- partial_bathrooms: string (nullable = true)\n",
      " |-- participant: string (nullable = true)\n",
      " |-- patio: string (nullable = true)\n",
      " |-- pool: string (nullable = true)\n",
      " |-- postal: string (nullable = true)\n",
      " |-- price_cents: string (nullable = true)\n",
      " |-- price_cents_sqft: string (nullable = true)\n",
      " |-- property_sub_type: string (nullable = true)\n",
      " |-- property_type: string (nullable = true)\n",
      " |-- provider_category: string (nullable = true)\n",
      " |-- provider_name: string (nullable = true)\n",
      " |-- provider_url: string (nullable = true)\n",
      " |-- province: string (nullable = true)\n",
      " |-- roof: string (nullable = true)\n",
      " |-- roof_types: string (nullable = true)\n",
      " |-- room_count: string (nullable = true)\n",
      " |-- rooms: string (nullable = true)\n",
      " |-- school: string (nullable = true)\n",
      " |-- scoring: string (nullable = true)\n",
      " |-- showing_date: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- stories: string (nullable = true)\n",
      " |-- tags: string (nullable = true)\n",
      " |-- tax_amount: string (nullable = true)\n",
      " |-- three_quarter_bathrooms: string (nullable = true)\n",
      " |-- total_sqft: string (nullable = true)\n",
      " |-- unit_number: string (nullable = true)\n",
      " |-- updated_at: string (nullable = true)\n",
      " |-- utilities: string (nullable = true)\n",
      " |-- water: string (nullable = true)\n",
      " |-- water_heater: string (nullable = true)\n",
      " |-- year_built: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o22464.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 31.0 failed 1 times, most recent failure: Lost task 0.0 in stage 31.0 (TID 31, localhost, executor driver): com.mongodb.spark.exceptions.MongoTypeConversionException: Cannot cast STRING into a NullType (value: BsonString{value='active'})\n\tat com.mongodb.spark.sql.MapFunctions$.com$mongodb$spark$sql$MapFunctions$$convertToDataType(MapFunctions.scala:200)\n\tat com.mongodb.spark.sql.MapFunctions$$anonfun$3.apply(MapFunctions.scala:39)\n\tat com.mongodb.spark.sql.MapFunctions$$anonfun$3.apply(MapFunctions.scala:37)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)\n\tat com.mongodb.spark.sql.MapFunctions$.documentToRow(MapFunctions.scala:37)\n\tat com.mongodb.spark.sql.MongoRelation$$anonfun$buildScan$1.apply(MongoRelation.scala:58)\n\tat com.mongodb.spark.sql.MongoRelation$$anonfun$buildScan$1.apply(MongoRelation.scala:58)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3383)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2758)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.GeneratedMethodAccessor65.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.mongodb.spark.exceptions.MongoTypeConversionException: Cannot cast STRING into a NullType (value: BsonString{value='active'})\n\tat com.mongodb.spark.sql.MapFunctions$.com$mongodb$spark$sql$MapFunctions$$convertToDataType(MapFunctions.scala:200)\n\tat com.mongodb.spark.sql.MapFunctions$$anonfun$3.apply(MapFunctions.scala:39)\n\tat com.mongodb.spark.sql.MapFunctions$$anonfun$3.apply(MapFunctions.scala:37)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)\n\tat com.mongodb.spark.sql.MapFunctions$.documentToRow(MapFunctions.scala:37)\n\tat com.mongodb.spark.sql.MongoRelation$$anonfun$buildScan$1.apply(MongoRelation.scala:58)\n\tat com.mongodb.spark.sql.MongoRelation$$anonfun$buildScan$1.apply(MongoRelation.scala:58)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-156-c41c520a0df3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"state\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \"\"\"\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o22464.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 31.0 failed 1 times, most recent failure: Lost task 0.0 in stage 31.0 (TID 31, localhost, executor driver): com.mongodb.spark.exceptions.MongoTypeConversionException: Cannot cast STRING into a NullType (value: BsonString{value='active'})\n\tat com.mongodb.spark.sql.MapFunctions$.com$mongodb$spark$sql$MapFunctions$$convertToDataType(MapFunctions.scala:200)\n\tat com.mongodb.spark.sql.MapFunctions$$anonfun$3.apply(MapFunctions.scala:39)\n\tat com.mongodb.spark.sql.MapFunctions$$anonfun$3.apply(MapFunctions.scala:37)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)\n\tat com.mongodb.spark.sql.MapFunctions$.documentToRow(MapFunctions.scala:37)\n\tat com.mongodb.spark.sql.MongoRelation$$anonfun$buildScan$1.apply(MongoRelation.scala:58)\n\tat com.mongodb.spark.sql.MongoRelation$$anonfun$buildScan$1.apply(MongoRelation.scala:58)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3383)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2758)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.GeneratedMethodAccessor65.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.mongodb.spark.exceptions.MongoTypeConversionException: Cannot cast STRING into a NullType (value: BsonString{value='active'})\n\tat com.mongodb.spark.sql.MapFunctions$.com$mongodb$spark$sql$MapFunctions$$convertToDataType(MapFunctions.scala:200)\n\tat com.mongodb.spark.sql.MapFunctions$$anonfun$3.apply(MapFunctions.scala:39)\n\tat com.mongodb.spark.sql.MapFunctions$$anonfun$3.apply(MapFunctions.scala:37)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)\n\tat com.mongodb.spark.sql.MapFunctions$.documentToRow(MapFunctions.scala:37)\n\tat com.mongodb.spark.sql.MongoRelation$$anonfun$buildScan$1.apply(MongoRelation.scala:58)\n\tat com.mongodb.spark.sql.MongoRelation$$anonfun$buildScan$1.apply(MongoRelation.scala:58)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "data_df.select(\"state\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- address_number: string (nullable = true)\n",
      " |-- address_street: string (nullable = true)\n",
      " |-- agent_avatar_url: null (nullable = true)\n",
      " |-- agent_email: string (nullable = true)\n",
      " |-- agent_external_id: string (nullable = true)\n",
      " |-- agent_fname: string (nullable = true)\n",
      " |-- agent_id: string (nullable = true)\n",
      " |-- agent_key: string (nullable = true)\n",
      " |-- agent_lname: string (nullable = true)\n",
      " |-- agent_name: string (nullable = true)\n",
      " |-- agent_phone: string (nullable = true)\n",
      " |-- architecture_style: string (nullable = true)\n",
      " |-- available_showing: null (nullable = true)\n",
      " |-- basement: boolean (nullable = true)\n",
      " |-- bathrooms: double (nullable = true)\n",
      " |-- beds: integer (nullable = true)\n",
      " |-- brokerage: struct (nullable = true)\n",
      " |    |-- address: struct (nullable = true)\n",
      " |    |    |-- commons:City: string (nullable = true)\n",
      " |    |    |-- commons:Country: string (nullable = true)\n",
      " |    |    |-- commons:FullStreetAddress: string (nullable = true)\n",
      " |    |    |-- commons:PostalCode: string (nullable = true)\n",
      " |    |    |-- commons:StateOrProvince: string (nullable = true)\n",
      " |    |    |-- commons:UnitNumber: string (nullable = true)\n",
      " |    |    |-- commons:address-preference-order: string (nullable = true)\n",
      " |    |    |-- commons:preference-order: string (nullable = true)\n",
      " |    |-- logo_url: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- phone: string (nullable = true)\n",
      " |    |-- website_url: string (nullable = true)\n",
      " |-- brokerage_email: string (nullable = true)\n",
      " |-- brokerage_external_id: string (nullable = true)\n",
      " |-- brokerage_logo_url: string (nullable = true)\n",
      " |-- brokerage_name: string (nullable = true)\n",
      " |-- brokerage_phone: string (nullable = true)\n",
      " |-- brokerage_website_url: string (nullable = true)\n",
      " |-- building_sqft: double (nullable = true)\n",
      " |-- building_type: string (nullable = true)\n",
      " |-- building_utilities: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- cooling_systems: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- cover_public_url: string (nullable = true)\n",
      " |-- created_at: timestamp (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- disclose_address: string (nullable = true)\n",
      " |-- exterior_types: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- external_id: string (nullable = true)\n",
      " |-- external_type: struct (nullable = true)\n",
      " |    |-- symbol: string (nullable = true)\n",
      " |-- external_url: string (nullable = true)\n",
      " |-- fees: null (nullable = true)\n",
      " |-- fireplaces: string (nullable = true)\n",
      " |-- floors: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- floors_number: null (nullable = true)\n",
      " |-- foreclosure: boolean (nullable = true)\n",
      " |-- full_baths: integer (nullable = true)\n",
      " |-- furnished: null (nullable = true)\n",
      " |-- garage: boolean (nullable = true)\n",
      " |-- geocoded: boolean (nullable = true)\n",
      " |-- half_baths: integer (nullable = true)\n",
      " |-- has_attic: null (nullable = true)\n",
      " |-- has_barbecue_area: boolean (nullable = true)\n",
      " |-- has_basement: boolean (nullable = true)\n",
      " |-- has_ceiling_fan: boolean (nullable = true)\n",
      " |-- has_deck: boolean (nullable = true)\n",
      " |-- has_disabled_access: boolean (nullable = true)\n",
      " |-- has_dock: boolean (nullable = true)\n",
      " |-- has_doorman: boolean (nullable = true)\n",
      " |-- has_double_pane_windows: boolean (nullable = true)\n",
      " |-- has_elevator: boolean (nullable = true)\n",
      " |-- has_fireplace: boolean (nullable = true)\n",
      " |-- has_garage: boolean (nullable = true)\n",
      " |-- has_garden: boolean (nullable = true)\n",
      " |-- has_gated_entry: boolean (nullable = true)\n",
      " |-- has_green_house: boolean (nullable = true)\n",
      " |-- has_hot_tub_spa: boolean (nullable = true)\n",
      " |-- has_jetted_bath_tub: boolean (nullable = true)\n",
      " |-- has_mother_in_law: boolean (nullable = true)\n",
      " |-- has_patio: boolean (nullable = true)\n",
      " |-- has_pond: boolean (nullable = true)\n",
      " |-- has_pool: boolean (nullable = true)\n",
      " |-- has_porch: boolean (nullable = true)\n",
      " |-- has_rv_parking: boolean (nullable = true)\n",
      " |-- has_sauna: boolean (nullable = true)\n",
      " |-- has_security_system: boolean (nullable = true)\n",
      " |-- has_skylight: boolean (nullable = true)\n",
      " |-- has_sports_court: boolean (nullable = true)\n",
      " |-- has_sprinkler_system: boolean (nullable = true)\n",
      " |-- has_vaulted_ceiling: boolean (nullable = true)\n",
      " |-- has_wet_bar: boolean (nullable = true)\n",
      " |-- heating_fuels: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- heating_systems: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- images: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- _id: struct (nullable = true)\n",
      " |    |    |    |-- oid: string (nullable = true)\n",
      " |    |    |-- created_at: timestamp (nullable = true)\n",
      " |    |    |-- modified_at: timestamp (nullable = true)\n",
      " |    |    |-- original_url: string (nullable = true)\n",
      " |    |    |-- sha1: string (nullable = true)\n",
      " |    |    |-- updated_at: timestamp (nullable = true)\n",
      " |    |    |-- url: string (nullable = true)\n",
      " |-- intercom: boolean (nullable = true)\n",
      " |-- is_cable_ready: boolean (nullable = true)\n",
      " |-- is_new_construction: boolean (nullable = true)\n",
      " |-- is_waterfront: boolean (nullable = true)\n",
      " |-- is_wired: boolean (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- lead_routing_email: string (nullable = true)\n",
      " |-- listing_category: string (nullable = true)\n",
      " |-- listing_date: timestamp (nullable = true)\n",
      " |-- listing_status: struct (nullable = true)\n",
      " |    |-- symbol: string (nullable = true)\n",
      " |-- listing_title: string (nullable = true)\n",
      " |-- listing_type: string (nullable = true)\n",
      " |-- listing_url: string (nullable = true)\n",
      " |-- location: struct (nullable = true)\n",
      " |    |-- type: string (nullable = true)\n",
      " |    |-- coordinates: array (nullable = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |-- location_ids: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- oid: string (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- lot_sqft: double (nullable = true)\n",
      " |-- lower_location_id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- main_level_sqft: null (nullable = true)\n",
      " |-- mls_id: string (nullable = true)\n",
      " |-- mls_name: string (nullable = true)\n",
      " |-- mls_number: string (nullable = true)\n",
      " |-- modified_at: timestamp (nullable = true)\n",
      " |-- num_floors: double (nullable = true)\n",
      " |-- one_quarter_bathrooms: integer (nullable = true)\n",
      " |-- open_house_count: null (nullable = true)\n",
      " |-- parking: string (nullable = true)\n",
      " |-- parking_types: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- partial_bathrooms: integer (nullable = true)\n",
      " |-- participant: struct (nullable = true)\n",
      " |    |-- id: string (nullable = true)\n",
      " |    |-- key: string (nullable = true)\n",
      " |    |-- first_name: string (nullable = true)\n",
      " |    |-- last_name: string (nullable = true)\n",
      " |    |-- email: string (nullable = true)\n",
      " |    |-- office_phone: string (nullable = true)\n",
      " |    |-- website_url: string (nullable = true)\n",
      " |    |-- primary_contact_phone: string (nullable = true)\n",
      " |-- patio: boolean (nullable = true)\n",
      " |-- pool: boolean (nullable = true)\n",
      " |-- postal: string (nullable = true)\n",
      " |-- price_cents: integer (nullable = true)\n",
      " |-- price_cents_sqft: integer (nullable = true)\n",
      " |-- property_sub_type: string (nullable = true)\n",
      " |-- property_type: string (nullable = true)\n",
      " |-- provider_category: string (nullable = true)\n",
      " |-- provider_name: string (nullable = true)\n",
      " |-- provider_url: string (nullable = true)\n",
      " |-- province: string (nullable = true)\n",
      " |-- roof: string (nullable = true)\n",
      " |-- roof_types: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- room_count: integer (nullable = true)\n",
      " |-- rooms: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- school: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- scoring: null (nullable = true)\n",
      " |-- showing_date: null (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- status: struct (nullable = true)\n",
      " |    |-- symbol: string (nullable = true)\n",
      " |-- stories: string (nullable = true)\n",
      " |-- tags: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- symbol: string (nullable = true)\n",
      " |-- tax_amount: string (nullable = true)\n",
      " |-- three_quarter_bathrooms: integer (nullable = true)\n",
      " |-- total_sqft: double (nullable = true)\n",
      " |-- unit_number: string (nullable = true)\n",
      " |-- updated_at: timestamp (nullable = true)\n",
      " |-- utilities: string (nullable = true)\n",
      " |-- water: string (nullable = true)\n",
      " |-- water_heater: string (nullable = true)\n",
      " |-- year_built: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df.columns:\n",
    "    data_df = df.withColumn(i, df[i].cast(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- address_number: string (nullable = true)\n",
      " |-- address_street: string (nullable = true)\n",
      " |-- agent_avatar_url: null (nullable = true)\n",
      " |-- agent_email: string (nullable = true)\n",
      " |-- agent_external_id: string (nullable = true)\n",
      " |-- agent_fname: string (nullable = true)\n",
      " |-- agent_id: string (nullable = true)\n",
      " |-- agent_key: string (nullable = true)\n",
      " |-- agent_lname: string (nullable = true)\n",
      " |-- agent_name: string (nullable = true)\n",
      " |-- agent_phone: string (nullable = true)\n",
      " |-- architecture_style: string (nullable = true)\n",
      " |-- available_showing: null (nullable = true)\n",
      " |-- basement: boolean (nullable = true)\n",
      " |-- bathrooms: double (nullable = true)\n",
      " |-- beds: integer (nullable = true)\n",
      " |-- brokerage: struct (nullable = true)\n",
      " |    |-- address: struct (nullable = true)\n",
      " |    |    |-- commons:City: string (nullable = true)\n",
      " |    |    |-- commons:Country: string (nullable = true)\n",
      " |    |    |-- commons:FullStreetAddress: string (nullable = true)\n",
      " |    |    |-- commons:PostalCode: string (nullable = true)\n",
      " |    |    |-- commons:StateOrProvince: string (nullable = true)\n",
      " |    |    |-- commons:UnitNumber: string (nullable = true)\n",
      " |    |    |-- commons:address-preference-order: string (nullable = true)\n",
      " |    |    |-- commons:preference-order: string (nullable = true)\n",
      " |    |-- logo_url: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- phone: string (nullable = true)\n",
      " |    |-- website_url: string (nullable = true)\n",
      " |-- brokerage_email: string (nullable = true)\n",
      " |-- brokerage_external_id: string (nullable = true)\n",
      " |-- brokerage_logo_url: string (nullable = true)\n",
      " |-- brokerage_name: string (nullable = true)\n",
      " |-- brokerage_phone: string (nullable = true)\n",
      " |-- brokerage_website_url: string (nullable = true)\n",
      " |-- building_sqft: double (nullable = true)\n",
      " |-- building_type: string (nullable = true)\n",
      " |-- building_utilities: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- cooling_systems: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- cover_public_url: string (nullable = true)\n",
      " |-- created_at: timestamp (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- disclose_address: string (nullable = true)\n",
      " |-- exterior_types: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- external_id: string (nullable = true)\n",
      " |-- external_type: struct (nullable = true)\n",
      " |    |-- symbol: string (nullable = true)\n",
      " |-- external_url: string (nullable = true)\n",
      " |-- fees: null (nullable = true)\n",
      " |-- fireplaces: string (nullable = true)\n",
      " |-- floors: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- floors_number: null (nullable = true)\n",
      " |-- foreclosure: boolean (nullable = true)\n",
      " |-- full_baths: integer (nullable = true)\n",
      " |-- furnished: null (nullable = true)\n",
      " |-- garage: boolean (nullable = true)\n",
      " |-- geocoded: boolean (nullable = true)\n",
      " |-- half_baths: integer (nullable = true)\n",
      " |-- has_attic: null (nullable = true)\n",
      " |-- has_barbecue_area: boolean (nullable = true)\n",
      " |-- has_basement: boolean (nullable = true)\n",
      " |-- has_ceiling_fan: boolean (nullable = true)\n",
      " |-- has_deck: boolean (nullable = true)\n",
      " |-- has_disabled_access: boolean (nullable = true)\n",
      " |-- has_dock: boolean (nullable = true)\n",
      " |-- has_doorman: boolean (nullable = true)\n",
      " |-- has_double_pane_windows: boolean (nullable = true)\n",
      " |-- has_elevator: boolean (nullable = true)\n",
      " |-- has_fireplace: boolean (nullable = true)\n",
      " |-- has_garage: boolean (nullable = true)\n",
      " |-- has_garden: boolean (nullable = true)\n",
      " |-- has_gated_entry: boolean (nullable = true)\n",
      " |-- has_green_house: boolean (nullable = true)\n",
      " |-- has_hot_tub_spa: boolean (nullable = true)\n",
      " |-- has_jetted_bath_tub: boolean (nullable = true)\n",
      " |-- has_mother_in_law: boolean (nullable = true)\n",
      " |-- has_patio: boolean (nullable = true)\n",
      " |-- has_pond: boolean (nullable = true)\n",
      " |-- has_pool: boolean (nullable = true)\n",
      " |-- has_porch: boolean (nullable = true)\n",
      " |-- has_rv_parking: boolean (nullable = true)\n",
      " |-- has_sauna: boolean (nullable = true)\n",
      " |-- has_security_system: boolean (nullable = true)\n",
      " |-- has_skylight: boolean (nullable = true)\n",
      " |-- has_sports_court: boolean (nullable = true)\n",
      " |-- has_sprinkler_system: boolean (nullable = true)\n",
      " |-- has_vaulted_ceiling: boolean (nullable = true)\n",
      " |-- has_wet_bar: boolean (nullable = true)\n",
      " |-- heating_fuels: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- heating_systems: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- images: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- _id: struct (nullable = true)\n",
      " |    |    |    |-- oid: string (nullable = true)\n",
      " |    |    |-- created_at: timestamp (nullable = true)\n",
      " |    |    |-- modified_at: timestamp (nullable = true)\n",
      " |    |    |-- original_url: string (nullable = true)\n",
      " |    |    |-- sha1: string (nullable = true)\n",
      " |    |    |-- updated_at: timestamp (nullable = true)\n",
      " |    |    |-- url: string (nullable = true)\n",
      " |-- intercom: boolean (nullable = true)\n",
      " |-- is_cable_ready: boolean (nullable = true)\n",
      " |-- is_new_construction: boolean (nullable = true)\n",
      " |-- is_waterfront: boolean (nullable = true)\n",
      " |-- is_wired: boolean (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- lead_routing_email: string (nullable = true)\n",
      " |-- listing_category: string (nullable = true)\n",
      " |-- listing_date: timestamp (nullable = true)\n",
      " |-- listing_status: struct (nullable = true)\n",
      " |    |-- symbol: string (nullable = true)\n",
      " |-- listing_title: string (nullable = true)\n",
      " |-- listing_type: string (nullable = true)\n",
      " |-- listing_url: string (nullable = true)\n",
      " |-- location: struct (nullable = true)\n",
      " |    |-- type: string (nullable = true)\n",
      " |    |-- coordinates: array (nullable = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |-- location_ids: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- oid: string (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- lot_sqft: double (nullable = true)\n",
      " |-- lower_location_id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- main_level_sqft: null (nullable = true)\n",
      " |-- mls_id: string (nullable = true)\n",
      " |-- mls_name: string (nullable = true)\n",
      " |-- mls_number: string (nullable = true)\n",
      " |-- modified_at: timestamp (nullable = true)\n",
      " |-- num_floors: double (nullable = true)\n",
      " |-- one_quarter_bathrooms: integer (nullable = true)\n",
      " |-- open_house_count: null (nullable = true)\n",
      " |-- parking: string (nullable = true)\n",
      " |-- parking_types: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- partial_bathrooms: integer (nullable = true)\n",
      " |-- participant: struct (nullable = true)\n",
      " |    |-- id: string (nullable = true)\n",
      " |    |-- key: string (nullable = true)\n",
      " |    |-- first_name: string (nullable = true)\n",
      " |    |-- last_name: string (nullable = true)\n",
      " |    |-- email: string (nullable = true)\n",
      " |    |-- office_phone: string (nullable = true)\n",
      " |    |-- website_url: string (nullable = true)\n",
      " |    |-- primary_contact_phone: string (nullable = true)\n",
      " |-- patio: boolean (nullable = true)\n",
      " |-- pool: boolean (nullable = true)\n",
      " |-- postal: string (nullable = true)\n",
      " |-- price_cents: integer (nullable = true)\n",
      " |-- price_cents_sqft: integer (nullable = true)\n",
      " |-- property_sub_type: string (nullable = true)\n",
      " |-- property_type: string (nullable = true)\n",
      " |-- provider_category: string (nullable = true)\n",
      " |-- provider_name: string (nullable = true)\n",
      " |-- provider_url: string (nullable = true)\n",
      " |-- province: string (nullable = true)\n",
      " |-- roof: string (nullable = true)\n",
      " |-- roof_types: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- room_count: integer (nullable = true)\n",
      " |-- rooms: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- school: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- scoring: null (nullable = true)\n",
      " |-- showing_date: null (nullable = true)\n",
      " |-- state: null (nullable = true)\n",
      " |-- status: struct (nullable = true)\n",
      " |    |-- symbol: string (nullable = true)\n",
      " |-- stories: null (nullable = true)\n",
      " |-- tags: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- symbol: string (nullable = true)\n",
      " |-- tax_amount: string (nullable = true)\n",
      " |-- three_quarter_bathrooms: integer (nullable = true)\n",
      " |-- total_sqft: double (nullable = true)\n",
      " |-- unit_number: string (nullable = true)\n",
      " |-- updated_at: timestamp (nullable = true)\n",
      " |-- utilities: string (nullable = true)\n",
      " |-- water: string (nullable = true)\n",
      " |-- water_heater: string (nullable = true)\n",
      " |-- year_built: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o21383.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 29.0 failed 1 times, most recent failure: Lost task 0.0 in stage 29.0 (TID 29, localhost, executor driver): com.mongodb.spark.exceptions.MongoTypeConversionException: Cannot cast BOOLEAN into a NullType (value: BsonBoolean{value=true})\n\tat com.mongodb.spark.sql.MapFunctions$.com$mongodb$spark$sql$MapFunctions$$convertToDataType(MapFunctions.scala:200)\n\tat com.mongodb.spark.sql.MapFunctions$$anonfun$3.apply(MapFunctions.scala:39)\n\tat com.mongodb.spark.sql.MapFunctions$$anonfun$3.apply(MapFunctions.scala:37)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)\n\tat com.mongodb.spark.sql.MapFunctions$.documentToRow(MapFunctions.scala:37)\n\tat com.mongodb.spark.sql.MongoRelation$$anonfun$buildScan$1.apply(MongoRelation.scala:58)\n\tat com.mongodb.spark.sql.MongoRelation$$anonfun$buildScan$1.apply(MongoRelation.scala:58)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:256)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3383)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2758)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.mongodb.spark.exceptions.MongoTypeConversionException: Cannot cast BOOLEAN into a NullType (value: BsonBoolean{value=true})\n\tat com.mongodb.spark.sql.MapFunctions$.com$mongodb$spark$sql$MapFunctions$$convertToDataType(MapFunctions.scala:200)\n\tat com.mongodb.spark.sql.MapFunctions$$anonfun$3.apply(MapFunctions.scala:39)\n\tat com.mongodb.spark.sql.MapFunctions$$anonfun$3.apply(MapFunctions.scala:37)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)\n\tat com.mongodb.spark.sql.MapFunctions$.documentToRow(MapFunctions.scala:37)\n\tat com.mongodb.spark.sql.MongoRelation$$anonfun$buildScan$1.apply(MongoRelation.scala:58)\n\tat com.mongodb.spark.sql.MongoRelation$$anonfun$buildScan$1.apply(MongoRelation.scala:58)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:256)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-130-f51d3e043cb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnew_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \"\"\"\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o21383.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 29.0 failed 1 times, most recent failure: Lost task 0.0 in stage 29.0 (TID 29, localhost, executor driver): com.mongodb.spark.exceptions.MongoTypeConversionException: Cannot cast BOOLEAN into a NullType (value: BsonBoolean{value=true})\n\tat com.mongodb.spark.sql.MapFunctions$.com$mongodb$spark$sql$MapFunctions$$convertToDataType(MapFunctions.scala:200)\n\tat com.mongodb.spark.sql.MapFunctions$$anonfun$3.apply(MapFunctions.scala:39)\n\tat com.mongodb.spark.sql.MapFunctions$$anonfun$3.apply(MapFunctions.scala:37)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)\n\tat com.mongodb.spark.sql.MapFunctions$.documentToRow(MapFunctions.scala:37)\n\tat com.mongodb.spark.sql.MongoRelation$$anonfun$buildScan$1.apply(MongoRelation.scala:58)\n\tat com.mongodb.spark.sql.MongoRelation$$anonfun$buildScan$1.apply(MongoRelation.scala:58)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:256)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3383)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2758)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.mongodb.spark.exceptions.MongoTypeConversionException: Cannot cast BOOLEAN into a NullType (value: BsonBoolean{value=true})\n\tat com.mongodb.spark.sql.MapFunctions$.com$mongodb$spark$sql$MapFunctions$$convertToDataType(MapFunctions.scala:200)\n\tat com.mongodb.spark.sql.MapFunctions$$anonfun$3.apply(MapFunctions.scala:39)\n\tat com.mongodb.spark.sql.MapFunctions$$anonfun$3.apply(MapFunctions.scala:37)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)\n\tat com.mongodb.spark.sql.MapFunctions$.documentToRow(MapFunctions.scala:37)\n\tat com.mongodb.spark.sql.MongoRelation$$anonfun$buildScan$1.apply(MongoRelation.scala:58)\n\tat com.mongodb.spark.sql.MongoRelation$$anonfun$buildScan$1.apply(MongoRelation.scala:58)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:256)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(_id,StructType(List(StructField(oid,StringType,true))),true),StructField(address_number,StringType,true),StructField(address_street,StringType,true),StructField(agent_avatar_url,NullType,true),StructField(agent_email,StringType,true),StructField(agent_external_id,StringType,true),StructField(agent_fname,StringType,true),StructField(agent_id,StringType,true),StructField(agent_key,StringType,true),StructField(agent_lname,StringType,true),StructField(agent_name,StringType,true),StructField(agent_phone,StringType,true),StructField(architecture_style,StringType,true),StructField(available_showing,NullType,true),StructField(basement,BooleanType,true),StructField(bathrooms,DoubleType,true),StructField(beds,IntegerType,true),StructField(brokerage,StructType(List(StructField(address,StructType(List(StructField(commons:City,StringType,true),StructField(commons:Country,StringType,true),StructField(commons:FullStreetAddress,StringType,true),StructField(commons:PostalCode,StringType,true),StructField(commons:StateOrProvince,StringType,true),StructField(commons:UnitNumber,StringType,true),StructField(commons:address-preference-order,StringType,true),StructField(commons:preference-order,StringType,true))),true),StructField(logo_url,StringType,true),StructField(name,StringType,true),StructField(phone,StringType,true),StructField(website_url,StringType,true))),true),StructField(brokerage_email,StringType,true),StructField(brokerage_external_id,StringType,true),StructField(brokerage_logo_url,StringType,true),StructField(brokerage_name,StringType,true),StructField(brokerage_phone,StringType,true),StructField(brokerage_website_url,StringType,true),StructField(building_sqft,DoubleType,true),StructField(building_type,StringType,true),StructField(building_utilities,StringType,true),StructField(city,StringType,true),StructField(cooling_systems,ArrayType(StringType,true),true),StructField(country,StringType,true),StructField(cover_public_url,StringType,true),StructField(created_at,TimestampType,true),StructField(description,StringType,true),StructField(disclose_address,StringType,true),StructField(exterior_types,ArrayType(StringType,true),true),StructField(external_id,StringType,true),StructField(external_type,StructType(List(StructField(symbol,StringType,true))),true),StructField(external_url,StringType,true),StructField(fees,NullType,true),StructField(fireplaces,StringType,true),StructField(floors,ArrayType(StringType,true),true),StructField(floors_number,NullType,true),StructField(foreclosure,BooleanType,true),StructField(full_baths,IntegerType,true),StructField(furnished,NullType,true),StructField(garage,BooleanType,true),StructField(geocoded,BooleanType,true),StructField(half_baths,IntegerType,true),StructField(has_attic,NullType,true),StructField(has_barbecue_area,BooleanType,true),StructField(has_basement,BooleanType,true),StructField(has_ceiling_fan,BooleanType,true),StructField(has_deck,BooleanType,true),StructField(has_disabled_access,BooleanType,true),StructField(has_dock,BooleanType,true),StructField(has_doorman,BooleanType,true),StructField(has_double_pane_windows,BooleanType,true),StructField(has_elevator,BooleanType,true),StructField(has_fireplace,BooleanType,true),StructField(has_garage,BooleanType,true),StructField(has_garden,BooleanType,true),StructField(has_gated_entry,BooleanType,true),StructField(has_green_house,BooleanType,true),StructField(has_hot_tub_spa,BooleanType,true),StructField(has_jetted_bath_tub,BooleanType,true),StructField(has_mother_in_law,BooleanType,true),StructField(has_patio,BooleanType,true),StructField(has_pond,BooleanType,true),StructField(has_pool,BooleanType,true),StructField(has_porch,BooleanType,true),StructField(has_rv_parking,BooleanType,true),StructField(has_sauna,BooleanType,true),StructField(has_security_system,BooleanType,true),StructField(has_skylight,BooleanType,true),StructField(has_sports_court,BooleanType,true),StructField(has_sprinkler_system,BooleanType,true),StructField(has_vaulted_ceiling,BooleanType,true),StructField(has_wet_bar,BooleanType,true),StructField(heating_fuels,ArrayType(StringType,true),true),StructField(heating_systems,ArrayType(StringType,true),true),StructField(images,ArrayType(StructType(List(StructField(_id,StructType(List(StructField(oid,StringType,true))),true),StructField(created_at,TimestampType,true),StructField(modified_at,TimestampType,true),StructField(original_url,StringType,true),StructField(sha1,StringType,true),StructField(updated_at,TimestampType,true),StructField(url,StringType,true))),true),true),StructField(intercom,BooleanType,true),StructField(is_cable_ready,BooleanType,true),StructField(is_new_construction,BooleanType,true),StructField(is_waterfront,BooleanType,true),StructField(is_wired,BooleanType,true),StructField(latitude,DoubleType,true),StructField(lead_routing_email,StringType,true),StructField(listing_category,StringType,true),StructField(listing_date,TimestampType,true),StructField(listing_status,StructType(List(StructField(symbol,StringType,true))),true),StructField(listing_title,StringType,true),StructField(listing_type,StringType,true),StructField(listing_url,StringType,true),StructField(location,StructType(List(StructField(type,StringType,true),StructField(coordinates,ArrayType(DoubleType,true),true))),true),StructField(location_ids,ArrayType(StructType(List(StructField(oid,StringType,true))),true),true),StructField(longitude,DoubleType,true),StructField(lot_sqft,DoubleType,true),StructField(lower_location_id,StructType(List(StructField(oid,StringType,true))),true),StructField(main_level_sqft,NullType,true),StructField(mls_id,StringType,true),StructField(mls_name,StringType,true),StructField(mls_number,StringType,true),StructField(modified_at,TimestampType,true),StructField(num_floors,DoubleType,true),StructField(one_quarter_bathrooms,IntegerType,true),StructField(open_house_count,NullType,true),StructField(parking,StringType,true),StructField(parking_types,ArrayType(StringType,true),true),StructField(partial_bathrooms,IntegerType,true),StructField(participant,StructType(List(StructField(id,StringType,true),StructField(key,StringType,true),StructField(first_name,StringType,true),StructField(last_name,StringType,true),StructField(email,StringType,true),StructField(office_phone,StringType,true),StructField(website_url,StringType,true),StructField(primary_contact_phone,StringType,true))),true),StructField(patio,BooleanType,true),StructField(pool,BooleanType,true),StructField(postal,StringType,true),StructField(price_cents,IntegerType,true),StructField(price_cents_sqft,IntegerType,true),StructField(property_sub_type,StringType,true),StructField(property_type,StringType,true),StructField(provider_category,StringType,true),StructField(provider_name,StringType,true),StructField(provider_url,StringType,true),StructField(province,StringType,true),StructField(roof,StringType,true),StructField(roof_types,ArrayType(StringType,true),true),StructField(room_count,IntegerType,true),StructField(rooms,ArrayType(StringType,true),true),StructField(school,ArrayType(StringType,true),true),StructField(scoring,NullType,true),StructField(showing_date,StringType,true),StructField(state,NullType,true),StructField(status,StructType(List(StructField(symbol,StringType,true))),true),StructField(stories,NullType,true),StructField(tags,ArrayType(StructType(List(StructField(symbol,StringType,true))),true),true),StructField(tax_amount,StringType,true),StructField(three_quarter_bathrooms,IntegerType,true),StructField(total_sqft,DoubleType,true),StructField(unit_number,StringType,true),StructField(updated_at,TimestampType,true),StructField(utilities,StringType,true),StructField(water,StringType,true),StructField(water_heater,StringType,true),StructField(year_built,IntegerType,true)))"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = df.withColumn(\"basement\", df[\"basement\"].cast(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[basement: boolean]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.select(\"basement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_id\n",
      "address_number\n",
      "address_street\n",
      "agent_avatar_url\n",
      "agent_email\n",
      "agent_external_id\n",
      "agent_fname\n",
      "agent_id\n",
      "agent_key\n",
      "agent_lname\n",
      "agent_name\n",
      "agent_phone\n",
      "architecture_style\n",
      "available_showing\n",
      "basement\n",
      "bathrooms\n",
      "beds\n",
      "brokerage\n",
      "brokerage_email\n",
      "brokerage_external_id\n",
      "brokerage_logo_url\n",
      "brokerage_name\n",
      "brokerage_phone\n",
      "brokerage_website_url\n",
      "building_sqft\n",
      "building_type\n",
      "building_utilities\n",
      "city\n",
      "cooling_systems\n",
      "country\n",
      "cover_public_url\n",
      "created_at\n",
      "description\n",
      "disclose_address\n",
      "exterior_types\n",
      "external_id\n",
      "external_type\n",
      "external_url\n",
      "fees\n",
      "fireplaces\n",
      "floors\n",
      "floors_number\n",
      "foreclosure\n",
      "full_baths\n",
      "furnished\n",
      "garage\n",
      "geocoded\n",
      "half_baths\n",
      "has_attic\n",
      "has_barbecue_area\n",
      "has_basement\n",
      "has_ceiling_fan\n",
      "has_deck\n",
      "has_disabled_access\n",
      "has_dock\n",
      "has_doorman\n",
      "has_double_pane_windows\n",
      "has_elevator\n",
      "has_fireplace\n",
      "has_garage\n",
      "has_garden\n",
      "has_gated_entry\n",
      "has_green_house\n",
      "has_hot_tub_spa\n",
      "has_jetted_bath_tub\n",
      "has_mother_in_law\n",
      "has_patio\n",
      "has_pond\n",
      "has_pool\n",
      "has_porch\n",
      "has_rv_parking\n",
      "has_sauna\n",
      "has_security_system\n",
      "has_skylight\n",
      "has_sports_court\n",
      "has_sprinkler_system\n",
      "has_vaulted_ceiling\n",
      "has_wet_bar\n",
      "heating_fuels\n",
      "heating_systems\n",
      "images\n",
      "intercom\n",
      "is_cable_ready\n",
      "is_new_construction\n",
      "is_waterfront\n",
      "is_wired\n",
      "latitude\n",
      "lead_routing_email\n",
      "listing_category\n",
      "listing_date\n",
      "listing_status\n",
      "listing_title\n",
      "listing_type\n",
      "listing_url\n",
      "living_area\n",
      "location\n",
      "location_ids\n",
      "longitude\n",
      "lot_sqft\n",
      "lower_location_id\n",
      "main_level_sqft\n",
      "mls_id\n",
      "mls_name\n",
      "mls_number\n",
      "modified_at\n",
      "num_floors\n",
      "one_quarter_bathrooms\n",
      "open_house_count\n",
      "parking\n",
      "parking_types\n",
      "partial_bathrooms\n",
      "participant\n",
      "patio\n",
      "pool\n",
      "postal\n",
      "price_cents\n",
      "price_cents_sqft\n",
      "property_sub_type\n",
      "property_type\n",
      "provider_category\n",
      "provider_name\n",
      "provider_url\n",
      "province\n",
      "roof\n",
      "roof_types\n",
      "room_count\n",
      "rooms\n",
      "school\n",
      "scoring\n",
      "showing_date\n",
      "state\n",
      "status\n",
      "stories\n",
      "tags\n",
      "tax_amount\n",
      "three_quarter_bathrooms\n",
      "total_sqft\n",
      "unit_number\n",
      "updated_at\n",
      "utilities\n",
      "water\n",
      "water_heater\n",
      "year_built\n"
     ]
    }
   ],
   "source": [
    "for i in df.columns:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- address_number: string (nullable = true)\n",
      " |-- address_street: string (nullable = true)\n",
      " |-- agent_avatar_url: null (nullable = true)\n",
      " |-- agent_email: string (nullable = true)\n",
      " |-- agent_external_id: string (nullable = true)\n",
      " |-- agent_fname: string (nullable = true)\n",
      " |-- agent_id: string (nullable = true)\n",
      " |-- agent_key: string (nullable = true)\n",
      " |-- agent_lname: string (nullable = true)\n",
      " |-- agent_name: string (nullable = true)\n",
      " |-- agent_phone: string (nullable = true)\n",
      " |-- architecture_style: string (nullable = true)\n",
      " |-- available_showing: null (nullable = true)\n",
      " |-- basement: string (nullable = true)\n",
      " |-- bathrooms: double (nullable = true)\n",
      " |-- beds: integer (nullable = true)\n",
      " |-- brokerage: struct (nullable = true)\n",
      " |    |-- address: struct (nullable = true)\n",
      " |    |    |-- commons:City: string (nullable = true)\n",
      " |    |    |-- commons:Country: string (nullable = true)\n",
      " |    |    |-- commons:FullStreetAddress: string (nullable = true)\n",
      " |    |    |-- commons:PostalCode: string (nullable = true)\n",
      " |    |    |-- commons:StateOrProvince: string (nullable = true)\n",
      " |    |    |-- commons:UnitNumber: string (nullable = true)\n",
      " |    |    |-- commons:address-preference-order: string (nullable = true)\n",
      " |    |    |-- commons:preference-order: string (nullable = true)\n",
      " |    |-- logo_url: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- phone: string (nullable = true)\n",
      " |    |-- website_url: string (nullable = true)\n",
      " |-- brokerage_email: string (nullable = true)\n",
      " |-- brokerage_external_id: string (nullable = true)\n",
      " |-- brokerage_logo_url: string (nullable = true)\n",
      " |-- brokerage_name: string (nullable = true)\n",
      " |-- brokerage_phone: string (nullable = true)\n",
      " |-- brokerage_website_url: string (nullable = true)\n",
      " |-- building_sqft: double (nullable = true)\n",
      " |-- building_type: string (nullable = true)\n",
      " |-- building_utilities: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- cooling_systems: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- cover_public_url: string (nullable = true)\n",
      " |-- created_at: timestamp (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- disclose_address: string (nullable = true)\n",
      " |-- exterior_types: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- external_id: string (nullable = true)\n",
      " |-- external_type: struct (nullable = true)\n",
      " |    |-- symbol: string (nullable = true)\n",
      " |-- external_url: string (nullable = true)\n",
      " |-- fees: null (nullable = true)\n",
      " |-- fireplaces: string (nullable = true)\n",
      " |-- floors: string (nullable = true)\n",
      " |-- floors_number: double (nullable = true)\n",
      " |-- foreclosure: boolean (nullable = true)\n",
      " |-- full_baths: integer (nullable = true)\n",
      " |-- furnished: null (nullable = true)\n",
      " |-- garage: boolean (nullable = true)\n",
      " |-- geocoded: boolean (nullable = true)\n",
      " |-- half_baths: integer (nullable = true)\n",
      " |-- has_attic: boolean (nullable = true)\n",
      " |-- has_barbecue_area: boolean (nullable = true)\n",
      " |-- has_basement: boolean (nullable = true)\n",
      " |-- has_ceiling_fan: boolean (nullable = true)\n",
      " |-- has_deck: boolean (nullable = true)\n",
      " |-- has_disabled_access: boolean (nullable = true)\n",
      " |-- has_dock: boolean (nullable = true)\n",
      " |-- has_doorman: boolean (nullable = true)\n",
      " |-- has_double_pane_windows: boolean (nullable = true)\n",
      " |-- has_elevator: boolean (nullable = true)\n",
      " |-- has_fireplace: boolean (nullable = true)\n",
      " |-- has_garage: boolean (nullable = true)\n",
      " |-- has_garden: boolean (nullable = true)\n",
      " |-- has_gated_entry: boolean (nullable = true)\n",
      " |-- has_green_house: boolean (nullable = true)\n",
      " |-- has_hot_tub_spa: boolean (nullable = true)\n",
      " |-- has_jetted_bath_tub: boolean (nullable = true)\n",
      " |-- has_mother_in_law: boolean (nullable = true)\n",
      " |-- has_patio: boolean (nullable = true)\n",
      " |-- has_pond: boolean (nullable = true)\n",
      " |-- has_pool: boolean (nullable = true)\n",
      " |-- has_porch: boolean (nullable = true)\n",
      " |-- has_rv_parking: boolean (nullable = true)\n",
      " |-- has_sauna: boolean (nullable = true)\n",
      " |-- has_security_system: boolean (nullable = true)\n",
      " |-- has_skylight: boolean (nullable = true)\n",
      " |-- has_sports_court: boolean (nullable = true)\n",
      " |-- has_sprinkler_system: boolean (nullable = true)\n",
      " |-- has_vaulted_ceiling: boolean (nullable = true)\n",
      " |-- has_wet_bar: boolean (nullable = true)\n",
      " |-- heating_fuels: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- heating_systems: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- images: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- _id: struct (nullable = true)\n",
      " |    |    |    |-- oid: string (nullable = true)\n",
      " |    |    |-- created_at: timestamp (nullable = true)\n",
      " |    |    |-- modified_at: timestamp (nullable = true)\n",
      " |    |    |-- original_url: string (nullable = true)\n",
      " |    |    |-- sha1: string (nullable = true)\n",
      " |    |    |-- updated_at: timestamp (nullable = true)\n",
      " |    |    |-- url: string (nullable = true)\n",
      " |-- intercom: boolean (nullable = true)\n",
      " |-- is_cable_ready: boolean (nullable = true)\n",
      " |-- is_new_construction: boolean (nullable = true)\n",
      " |-- is_waterfront: boolean (nullable = true)\n",
      " |-- is_wired: boolean (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- lead_routing_email: string (nullable = true)\n",
      " |-- listing_category: string (nullable = true)\n",
      " |-- listing_date: timestamp (nullable = true)\n",
      " |-- listing_status: struct (nullable = true)\n",
      " |    |-- symbol: string (nullable = true)\n",
      " |-- listing_title: string (nullable = true)\n",
      " |-- listing_type: string (nullable = true)\n",
      " |-- listing_url: string (nullable = true)\n",
      " |-- living_area: string (nullable = true)\n",
      " |-- location: struct (nullable = true)\n",
      " |    |-- type: string (nullable = true)\n",
      " |    |-- coordinates: array (nullable = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |-- location_ids: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- oid: string (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- lot_sqft: double (nullable = true)\n",
      " |-- lower_location_id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- main_level_sqft: double (nullable = true)\n",
      " |-- mls_id: string (nullable = true)\n",
      " |-- mls_name: string (nullable = true)\n",
      " |-- mls_number: string (nullable = true)\n",
      " |-- modified_at: timestamp (nullable = true)\n",
      " |-- num_floors: double (nullable = true)\n",
      " |-- one_quarter_bathrooms: integer (nullable = true)\n",
      " |-- open_house_count: null (nullable = true)\n",
      " |-- parking: string (nullable = true)\n",
      " |-- parking_types: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- partial_bathrooms: integer (nullable = true)\n",
      " |-- participant: struct (nullable = true)\n",
      " |    |-- email: string (nullable = true)\n",
      " |    |-- first_name: string (nullable = true)\n",
      " |    |-- id: string (nullable = true)\n",
      " |    |-- key: string (nullable = true)\n",
      " |    |-- last_name: string (nullable = true)\n",
      " |    |-- office_phone: string (nullable = true)\n",
      " |    |-- primary_contact_phone: string (nullable = true)\n",
      " |    |-- website_url: string (nullable = true)\n",
      " |-- patio: boolean (nullable = true)\n",
      " |-- pool: boolean (nullable = true)\n",
      " |-- postal: string (nullable = true)\n",
      " |-- price_cents: integer (nullable = true)\n",
      " |-- price_cents_sqft: integer (nullable = true)\n",
      " |-- property_sub_type: string (nullable = true)\n",
      " |-- property_type: string (nullable = true)\n",
      " |-- provider_category: string (nullable = true)\n",
      " |-- provider_name: string (nullable = true)\n",
      " |-- provider_url: string (nullable = true)\n",
      " |-- province: string (nullable = true)\n",
      " |-- roof: string (nullable = true)\n",
      " |-- roof_types: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- room_count: integer (nullable = true)\n",
      " |-- rooms: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- school: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- scoring: null (nullable = true)\n",
      " |-- showing_date: null (nullable = true)\n",
      " |-- state: null (nullable = true)\n",
      " |-- status: struct (nullable = true)\n",
      " |    |-- symbol: string (nullable = true)\n",
      " |-- stories: integer (nullable = true)\n",
      " |-- tags: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- symbol: string (nullable = true)\n",
      " |-- tax_amount: string (nullable = true)\n",
      " |-- three_quarter_bathrooms: integer (nullable = true)\n",
      " |-- total_sqft: double (nullable = true)\n",
      " |-- unit_number: string (nullable = true)\n",
      " |-- updated_at: timestamp (nullable = true)\n",
      " |-- utilities: string (nullable = true)\n",
      " |-- water: string (nullable = true)\n",
      " |-- water_heater: string (nullable = true)\n",
      " |-- year_built: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "withColumn() missing 1 required positional argument: 'col'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-131b5aeef04e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;34m[\u001b[0m\u001b[0;34m\"state\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"state\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStringType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: withColumn() missing 1 required positional argument: 'col'"
     ]
    }
   ],
   "source": [
    "data_df = df.withColumn(  [\"state\", df[\"state\"].cast(StringType())] )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- address_number: string (nullable = true)\n",
      " |-- address_street: string (nullable = true)\n",
      " |-- agent_avatar_url: null (nullable = true)\n",
      " |-- agent_email: string (nullable = true)\n",
      " |-- agent_external_id: string (nullable = true)\n",
      " |-- agent_fname: string (nullable = true)\n",
      " |-- agent_id: string (nullable = true)\n",
      " |-- agent_key: string (nullable = true)\n",
      " |-- agent_lname: string (nullable = true)\n",
      " |-- agent_name: string (nullable = true)\n",
      " |-- agent_phone: string (nullable = true)\n",
      " |-- architecture_style: string (nullable = true)\n",
      " |-- available_showing: null (nullable = true)\n",
      " |-- basement: boolean (nullable = true)\n",
      " |-- bathrooms: double (nullable = true)\n",
      " |-- beds: integer (nullable = true)\n",
      " |-- brokerage: struct (nullable = true)\n",
      " |    |-- address: struct (nullable = true)\n",
      " |    |    |-- commons:City: string (nullable = true)\n",
      " |    |    |-- commons:Country: string (nullable = true)\n",
      " |    |    |-- commons:FullStreetAddress: string (nullable = true)\n",
      " |    |    |-- commons:PostalCode: string (nullable = true)\n",
      " |    |    |-- commons:StateOrProvince: string (nullable = true)\n",
      " |    |    |-- commons:UnitNumber: string (nullable = true)\n",
      " |    |    |-- commons:address-preference-order: string (nullable = true)\n",
      " |    |    |-- commons:preference-order: string (nullable = true)\n",
      " |    |-- logo_url: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |    |-- phone: string (nullable = true)\n",
      " |    |-- website_url: string (nullable = true)\n",
      " |-- brokerage_email: string (nullable = true)\n",
      " |-- brokerage_external_id: string (nullable = true)\n",
      " |-- brokerage_logo_url: string (nullable = true)\n",
      " |-- brokerage_name: string (nullable = true)\n",
      " |-- brokerage_phone: string (nullable = true)\n",
      " |-- brokerage_website_url: string (nullable = true)\n",
      " |-- building_sqft: double (nullable = true)\n",
      " |-- building_type: string (nullable = true)\n",
      " |-- building_utilities: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- cooling_systems: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- cover_public_url: string (nullable = true)\n",
      " |-- created_at: timestamp (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- disclose_address: string (nullable = true)\n",
      " |-- exterior_types: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- external_id: string (nullable = true)\n",
      " |-- external_type: struct (nullable = true)\n",
      " |    |-- symbol: string (nullable = true)\n",
      " |-- external_url: string (nullable = true)\n",
      " |-- fees: null (nullable = true)\n",
      " |-- fireplaces: string (nullable = true)\n",
      " |-- floors: string (nullable = true)\n",
      " |-- floors_number: double (nullable = true)\n",
      " |-- foreclosure: boolean (nullable = true)\n",
      " |-- full_baths: integer (nullable = true)\n",
      " |-- furnished: null (nullable = true)\n",
      " |-- garage: boolean (nullable = true)\n",
      " |-- geocoded: boolean (nullable = true)\n",
      " |-- half_baths: integer (nullable = true)\n",
      " |-- has_attic: boolean (nullable = true)\n",
      " |-- has_barbecue_area: boolean (nullable = true)\n",
      " |-- has_basement: boolean (nullable = true)\n",
      " |-- has_ceiling_fan: boolean (nullable = true)\n",
      " |-- has_deck: boolean (nullable = true)\n",
      " |-- has_disabled_access: boolean (nullable = true)\n",
      " |-- has_dock: boolean (nullable = true)\n",
      " |-- has_doorman: boolean (nullable = true)\n",
      " |-- has_double_pane_windows: boolean (nullable = true)\n",
      " |-- has_elevator: boolean (nullable = true)\n",
      " |-- has_fireplace: boolean (nullable = true)\n",
      " |-- has_garage: boolean (nullable = true)\n",
      " |-- has_garden: boolean (nullable = true)\n",
      " |-- has_gated_entry: boolean (nullable = true)\n",
      " |-- has_green_house: boolean (nullable = true)\n",
      " |-- has_hot_tub_spa: boolean (nullable = true)\n",
      " |-- has_jetted_bath_tub: boolean (nullable = true)\n",
      " |-- has_mother_in_law: boolean (nullable = true)\n",
      " |-- has_patio: boolean (nullable = true)\n",
      " |-- has_pond: boolean (nullable = true)\n",
      " |-- has_pool: boolean (nullable = true)\n",
      " |-- has_porch: boolean (nullable = true)\n",
      " |-- has_rv_parking: boolean (nullable = true)\n",
      " |-- has_sauna: boolean (nullable = true)\n",
      " |-- has_security_system: boolean (nullable = true)\n",
      " |-- has_skylight: boolean (nullable = true)\n",
      " |-- has_sports_court: boolean (nullable = true)\n",
      " |-- has_sprinkler_system: boolean (nullable = true)\n",
      " |-- has_vaulted_ceiling: boolean (nullable = true)\n",
      " |-- has_wet_bar: boolean (nullable = true)\n",
      " |-- heating_fuels: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- heating_systems: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- images: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- _id: struct (nullable = true)\n",
      " |    |    |    |-- oid: string (nullable = true)\n",
      " |    |    |-- created_at: timestamp (nullable = true)\n",
      " |    |    |-- modified_at: timestamp (nullable = true)\n",
      " |    |    |-- original_url: string (nullable = true)\n",
      " |    |    |-- sha1: string (nullable = true)\n",
      " |    |    |-- updated_at: timestamp (nullable = true)\n",
      " |    |    |-- url: string (nullable = true)\n",
      " |-- intercom: boolean (nullable = true)\n",
      " |-- is_cable_ready: boolean (nullable = true)\n",
      " |-- is_new_construction: boolean (nullable = true)\n",
      " |-- is_waterfront: boolean (nullable = true)\n",
      " |-- is_wired: boolean (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- lead_routing_email: string (nullable = true)\n",
      " |-- listing_category: string (nullable = true)\n",
      " |-- listing_date: timestamp (nullable = true)\n",
      " |-- listing_status: struct (nullable = true)\n",
      " |    |-- symbol: string (nullable = true)\n",
      " |-- listing_title: string (nullable = true)\n",
      " |-- listing_type: string (nullable = true)\n",
      " |-- listing_url: string (nullable = true)\n",
      " |-- living_area: string (nullable = true)\n",
      " |-- location: struct (nullable = true)\n",
      " |    |-- type: string (nullable = true)\n",
      " |    |-- coordinates: array (nullable = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |-- location_ids: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- oid: string (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- lot_sqft: double (nullable = true)\n",
      " |-- lower_location_id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- main_level_sqft: double (nullable = true)\n",
      " |-- mls_id: string (nullable = true)\n",
      " |-- mls_name: string (nullable = true)\n",
      " |-- mls_number: string (nullable = true)\n",
      " |-- modified_at: timestamp (nullable = true)\n",
      " |-- num_floors: double (nullable = true)\n",
      " |-- one_quarter_bathrooms: integer (nullable = true)\n",
      " |-- open_house_count: null (nullable = true)\n",
      " |-- parking: string (nullable = true)\n",
      " |-- parking_types: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- partial_bathrooms: integer (nullable = true)\n",
      " |-- participant: struct (nullable = true)\n",
      " |    |-- email: string (nullable = true)\n",
      " |    |-- first_name: string (nullable = true)\n",
      " |    |-- id: string (nullable = true)\n",
      " |    |-- key: string (nullable = true)\n",
      " |    |-- last_name: string (nullable = true)\n",
      " |    |-- office_phone: string (nullable = true)\n",
      " |    |-- primary_contact_phone: string (nullable = true)\n",
      " |    |-- website_url: string (nullable = true)\n",
      " |-- patio: boolean (nullable = true)\n",
      " |-- pool: boolean (nullable = true)\n",
      " |-- postal: string (nullable = true)\n",
      " |-- price_cents: integer (nullable = true)\n",
      " |-- price_cents_sqft: integer (nullable = true)\n",
      " |-- property_sub_type: string (nullable = true)\n",
      " |-- property_type: string (nullable = true)\n",
      " |-- provider_category: string (nullable = true)\n",
      " |-- provider_name: string (nullable = true)\n",
      " |-- provider_url: string (nullable = true)\n",
      " |-- province: string (nullable = true)\n",
      " |-- roof: string (nullable = true)\n",
      " |-- roof_types: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- room_count: integer (nullable = true)\n",
      " |-- rooms: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- school: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- scoring: null (nullable = true)\n",
      " |-- showing_date: null (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- status: struct (nullable = true)\n",
      " |    |-- symbol: string (nullable = true)\n",
      " |-- stories: integer (nullable = true)\n",
      " |-- tags: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- symbol: string (nullable = true)\n",
      " |-- tax_amount: string (nullable = true)\n",
      " |-- three_quarter_bathrooms: integer (nullable = true)\n",
      " |-- total_sqft: double (nullable = true)\n",
      " |-- unit_number: string (nullable = true)\n",
      " |-- updated_at: timestamp (nullable = true)\n",
      " |-- utilities: string (nullable = true)\n",
      " |-- water: string (nullable = true)\n",
      " |-- water_heater: string (nullable = true)\n",
      " |-- year_built: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-91-8c49985b239e>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-91-8c49985b239e>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    'building_sqft' - Cannot cast STRING into a DoubleType (value: BsonString{value=''})\u001b[0m\n\u001b[0m                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o922.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 23.0 failed 1 times, most recent failure: Lost task 0.0 in stage 23.0 (TID 23, localhost, executor driver): com.mongodb.spark.exceptions.MongoTypeConversionException: Cannot cast BOOLEAN into a NullType (value: BsonBoolean{value=true})\n\tat com.mongodb.spark.sql.MapFunctions$.com$mongodb$spark$sql$MapFunctions$$convertToDataType(MapFunctions.scala:200)\n\tat com.mongodb.spark.sql.MapFunctions$$anonfun$3.apply(MapFunctions.scala:39)\n\tat com.mongodb.spark.sql.MapFunctions$$anonfun$3.apply(MapFunctions.scala:37)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)\n\tat com.mongodb.spark.sql.MapFunctions$.documentToRow(MapFunctions.scala:37)\n\tat com.mongodb.spark.sql.MongoRelation$$anonfun$buildScan$1.apply(MongoRelation.scala:58)\n\tat com.mongodb.spark.sql.MongoRelation$$anonfun$buildScan$1.apply(MongoRelation.scala:58)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3383)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2758)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.mongodb.spark.exceptions.MongoTypeConversionException: Cannot cast BOOLEAN into a NullType (value: BsonBoolean{value=true})\n\tat com.mongodb.spark.sql.MapFunctions$.com$mongodb$spark$sql$MapFunctions$$convertToDataType(MapFunctions.scala:200)\n\tat com.mongodb.spark.sql.MapFunctions$$anonfun$3.apply(MapFunctions.scala:39)\n\tat com.mongodb.spark.sql.MapFunctions$$anonfun$3.apply(MapFunctions.scala:37)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)\n\tat com.mongodb.spark.sql.MapFunctions$.documentToRow(MapFunctions.scala:37)\n\tat com.mongodb.spark.sql.MongoRelation$$anonfun$buildScan$1.apply(MongoRelation.scala:58)\n\tat com.mongodb.spark.sql.MongoRelation$$anonfun$buildScan$1.apply(MongoRelation.scala:58)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-fa14abada454>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf01\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"available_showing\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \"\"\"\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o922.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 23.0 failed 1 times, most recent failure: Lost task 0.0 in stage 23.0 (TID 23, localhost, executor driver): com.mongodb.spark.exceptions.MongoTypeConversionException: Cannot cast BOOLEAN into a NullType (value: BsonBoolean{value=true})\n\tat com.mongodb.spark.sql.MapFunctions$.com$mongodb$spark$sql$MapFunctions$$convertToDataType(MapFunctions.scala:200)\n\tat com.mongodb.spark.sql.MapFunctions$$anonfun$3.apply(MapFunctions.scala:39)\n\tat com.mongodb.spark.sql.MapFunctions$$anonfun$3.apply(MapFunctions.scala:37)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)\n\tat com.mongodb.spark.sql.MapFunctions$.documentToRow(MapFunctions.scala:37)\n\tat com.mongodb.spark.sql.MongoRelation$$anonfun$buildScan$1.apply(MongoRelation.scala:58)\n\tat com.mongodb.spark.sql.MongoRelation$$anonfun$buildScan$1.apply(MongoRelation.scala:58)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3383)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2758)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.mongodb.spark.exceptions.MongoTypeConversionException: Cannot cast BOOLEAN into a NullType (value: BsonBoolean{value=true})\n\tat com.mongodb.spark.sql.MapFunctions$.com$mongodb$spark$sql$MapFunctions$$convertToDataType(MapFunctions.scala:200)\n\tat com.mongodb.spark.sql.MapFunctions$$anonfun$3.apply(MapFunctions.scala:39)\n\tat com.mongodb.spark.sql.MapFunctions$$anonfun$3.apply(MapFunctions.scala:37)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)\n\tat com.mongodb.spark.sql.MapFunctions$.documentToRow(MapFunctions.scala:37)\n\tat com.mongodb.spark.sql.MongoRelation$$anonfun$buildScan$1.apply(MongoRelation.scala:58)\n\tat com.mongodb.spark.sql.MongoRelation$$anonfun$buildScan$1.apply(MongoRelation.scala:58)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "df01.select(\"available_showing\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "schemaa=df01.schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-33ff619b1c08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mschemaa\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mStructType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_schema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/types.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fields)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfields\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStructField\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0;34m\"fields should be a list of StructField\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/types.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfields\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStructField\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0;34m\"fields should be a list of StructField\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'name'"
     ]
    }
   ],
   "source": [
    "schemaa= StructType(fields=data_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df01.withColumn('state', F.col('Total').cast(types.DecimalType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df01.registerTempTable(\"table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-21-158eb0902f37>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-21-158eb0902f37>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    df01.select(\"state\").cast(state).as(\"string\").show()\u001b[0m\n\u001b[0m                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "df01.select(\"state\").cast(state).as(\"string\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o259.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 11.0 failed 1 times, most recent failure: Lost task 0.0 in stage 11.0 (TID 780, localhost, executor driver): com.mongodb.spark.exceptions.MongoTypeConversionException: Cannot cast STRING into a NullType (value: BsonString{value='active'})\n\tat com.mongodb.spark.sql.MapFunctions$.com$mongodb$spark$sql$MapFunctions$$convertToDataType(MapFunctions.scala:200)\n\tat com.mongodb.spark.sql.MapFunctions$$anonfun$3.apply(MapFunctions.scala:39)\n\tat com.mongodb.spark.sql.MapFunctions$$anonfun$3.apply(MapFunctions.scala:37)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)\n\tat com.mongodb.spark.sql.MapFunctions$.documentToRow(MapFunctions.scala:37)\n\tat com.mongodb.spark.sql.MongoRelation$$anonfun$buildScan$1.apply(MongoRelation.scala:58)\n\tat com.mongodb.spark.sql.MongoRelation$$anonfun$buildScan$1.apply(MongoRelation.scala:58)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:299)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3257)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3254)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3254)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.mongodb.spark.exceptions.MongoTypeConversionException: Cannot cast STRING into a NullType (value: BsonString{value='active'})\n\tat com.mongodb.spark.sql.MapFunctions$.com$mongodb$spark$sql$MapFunctions$$convertToDataType(MapFunctions.scala:200)\n\tat com.mongodb.spark.sql.MapFunctions$$anonfun$3.apply(MapFunctions.scala:39)\n\tat com.mongodb.spark.sql.MapFunctions$$anonfun$3.apply(MapFunctions.scala:37)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)\n\tat com.mongodb.spark.sql.MapFunctions$.documentToRow(MapFunctions.scala:37)\n\tat com.mongodb.spark.sql.MongoRelation$$anonfun$buildScan$1.apply(MongoRelation.scala:58)\n\tat com.mongodb.spark.sql.MongoRelation$$anonfun$buildScan$1.apply(MongoRelation.scala:58)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-18706a1a7aeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf01\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf01\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"string\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'statesss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    531\u001b[0m         \"\"\"\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o259.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 11.0 failed 1 times, most recent failure: Lost task 0.0 in stage 11.0 (TID 780, localhost, executor driver): com.mongodb.spark.exceptions.MongoTypeConversionException: Cannot cast STRING into a NullType (value: BsonString{value='active'})\n\tat com.mongodb.spark.sql.MapFunctions$.com$mongodb$spark$sql$MapFunctions$$convertToDataType(MapFunctions.scala:200)\n\tat com.mongodb.spark.sql.MapFunctions$$anonfun$3.apply(MapFunctions.scala:39)\n\tat com.mongodb.spark.sql.MapFunctions$$anonfun$3.apply(MapFunctions.scala:37)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)\n\tat com.mongodb.spark.sql.MapFunctions$.documentToRow(MapFunctions.scala:37)\n\tat com.mongodb.spark.sql.MongoRelation$$anonfun$buildScan$1.apply(MongoRelation.scala:58)\n\tat com.mongodb.spark.sql.MongoRelation$$anonfun$buildScan$1.apply(MongoRelation.scala:58)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:299)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3257)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3254)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3254)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.mongodb.spark.exceptions.MongoTypeConversionException: Cannot cast STRING into a NullType (value: BsonString{value='active'})\n\tat com.mongodb.spark.sql.MapFunctions$.com$mongodb$spark$sql$MapFunctions$$convertToDataType(MapFunctions.scala:200)\n\tat com.mongodb.spark.sql.MapFunctions$$anonfun$3.apply(MapFunctions.scala:39)\n\tat com.mongodb.spark.sql.MapFunctions$$anonfun$3.apply(MapFunctions.scala:37)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)\n\tat com.mongodb.spark.sql.MapFunctions$.documentToRow(MapFunctions.scala:37)\n\tat com.mongodb.spark.sql.MongoRelation$$anonfun$buildScan$1.apply(MongoRelation.scala:58)\n\tat com.mongodb.spark.sql.MongoRelation$$anonfun$buildScan$1.apply(MongoRelation.scala:58)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:410)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "df01.select(df01.state.cast(\"string\").alias('statesss')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "count() takes exactly one argument (0 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-42bebe7a06b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: count() takes exactly one argument (0 given)"
     ]
    }
   ],
   "source": [
    "df_id.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "#client = MongoClient()\n",
    "db = client.backend_production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_us=db.properties.find( {\"country\": \"US\"}).limit(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(dd_us, orient='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>address_number</th>\n",
       "      <th>address_street</th>\n",
       "      <th>agent_avatar_url</th>\n",
       "      <th>agent_email</th>\n",
       "      <th>agent_external_id</th>\n",
       "      <th>agent_fname</th>\n",
       "      <th>agent_id</th>\n",
       "      <th>agent_key</th>\n",
       "      <th>agent_lname</th>\n",
       "      <th>...</th>\n",
       "      <th>tags</th>\n",
       "      <th>tax_amount</th>\n",
       "      <th>three_quarter_bathrooms</th>\n",
       "      <th>total_sqft</th>\n",
       "      <th>unit_number</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>utilities</th>\n",
       "      <th>water</th>\n",
       "      <th>water_heater</th>\n",
       "      <th>year_built</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5a3ceff1aed0f133b9456d98</td>\n",
       "      <td>2501</td>\n",
       "      <td>Avery Street</td>\n",
       "      <td>NaN</td>\n",
       "      <td>stephen@chalkandgibbs.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Stephen</td>\n",
       "      <td>20151206221506028181000000</td>\n",
       "      <td>3yd-CCMLSNC-20151206221506028181000000</td>\n",
       "      <td>W. Chalk Jr.</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2019-04-06 08:42:45.705</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1973.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5a3ceff9aed0f133b9456f3c</td>\n",
       "      <td>19</td>\n",
       "      <td>Privateer Drive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wheland@hargray.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tom</td>\n",
       "      <td>wheland</td>\n",
       "      <td>3yd-BCARSC-wheland</td>\n",
       "      <td>Wheland</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2019-04-06 08:42:45.773</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1986.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5a3cf088aed0f133b945837f</td>\n",
       "      <td>1701</td>\n",
       "      <td>Salter Path Road</td>\n",
       "      <td>NaN</td>\n",
       "      <td>carolyn@coastland.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Carolyn</td>\n",
       "      <td>20151206221518063769000000</td>\n",
       "      <td>3yd-CCMLSNC-20151206221518063769000000</td>\n",
       "      <td>Wood</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I204</td>\n",
       "      <td>2019-04-07 20:44:22.938</td>\n",
       "      <td>[Dishwasher, Dryer, Microwave, Oven, Refrigera...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2003.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5a3cf08aaed0f133b9458575</td>\n",
       "      <td>203</td>\n",
       "      <td>Bluewater Cove</td>\n",
       "      <td>NaN</td>\n",
       "      <td>roy@coastland.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Roy</td>\n",
       "      <td>20151206221451745345000000</td>\n",
       "      <td>3yd-CCMLSNC-20151206221451745345000000</td>\n",
       "      <td>Parker</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2019-04-06 08:42:45.951</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5a3cf0abaed0f133b9458be6</td>\n",
       "      <td>11</td>\n",
       "      <td>Shults Road</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jimlivingstonrealestate@gmail.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>James</td>\n",
       "      <td>livingstonj</td>\n",
       "      <td>3yd-BCARSC-livingstonj</td>\n",
       "      <td>R. Livingston</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2019-04-06 08:42:46.032</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1957.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 142 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        _id address_number    address_street  \\\n",
       "0  5a3ceff1aed0f133b9456d98           2501      Avery Street   \n",
       "1  5a3ceff9aed0f133b9456f3c             19   Privateer Drive   \n",
       "2  5a3cf088aed0f133b945837f           1701  Salter Path Road   \n",
       "3  5a3cf08aaed0f133b9458575            203    Bluewater Cove   \n",
       "4  5a3cf0abaed0f133b9458be6             11       Shults Road   \n",
       "\n",
       "   agent_avatar_url                        agent_email  agent_external_id  \\\n",
       "0               NaN          stephen@chalkandgibbs.com                NaN   \n",
       "1               NaN                wheland@hargray.com                NaN   \n",
       "2               NaN              carolyn@coastland.com                NaN   \n",
       "3               NaN                  roy@coastland.com                NaN   \n",
       "4               NaN  jimlivingstonrealestate@gmail.com                NaN   \n",
       "\n",
       "  agent_fname                    agent_id  \\\n",
       "0     Stephen  20151206221506028181000000   \n",
       "1         Tom                     wheland   \n",
       "2     Carolyn  20151206221518063769000000   \n",
       "3         Roy  20151206221451745345000000   \n",
       "4       James                 livingstonj   \n",
       "\n",
       "                                agent_key    agent_lname  ...  tags  \\\n",
       "0  3yd-CCMLSNC-20151206221506028181000000   W. Chalk Jr.  ...  None   \n",
       "1                      3yd-BCARSC-wheland        Wheland  ...  None   \n",
       "2  3yd-CCMLSNC-20151206221518063769000000           Wood  ...  None   \n",
       "3  3yd-CCMLSNC-20151206221451745345000000         Parker  ...  None   \n",
       "4                  3yd-BCARSC-livingstonj  R. Livingston  ...  None   \n",
       "\n",
       "  tax_amount three_quarter_bathrooms  total_sqft unit_number  \\\n",
       "0       None                     0.0         NaN        None   \n",
       "1       None                     0.0         NaN        None   \n",
       "2       None                     0.0         NaN        I204   \n",
       "3       None                     0.0         NaN        None   \n",
       "4       None                     0.0         NaN        None   \n",
       "\n",
       "               updated_at                                          utilities  \\\n",
       "0 2019-04-06 08:42:45.705                                               None   \n",
       "1 2019-04-06 08:42:45.773                                               None   \n",
       "2 2019-04-07 20:44:22.938  [Dishwasher, Dryer, Microwave, Oven, Refrigera...   \n",
       "3 2019-04-06 08:42:45.951                                               None   \n",
       "4 2019-04-06 08:42:46.032                                               None   \n",
       "\n",
       "  water water_heater year_built  \n",
       "0   NaN          NaN     1973.0  \n",
       "1   NaN          NaN     1986.0  \n",
       "2   NaN          NaN     2003.0  \n",
       "3   NaN          NaN     2018.0  \n",
       "4   NaN          NaN     1957.0  \n",
       "\n",
       "[5 rows x 142 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data_notebook.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "select() takes from 2 to 3 positional arguments but 135 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-cd2e7380ff92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'address_number'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'address_street'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'agent_avatar_url'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'agent_email'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'agent_external_id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'agent_fname'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'agent_id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'agent_key'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'agent_lname'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'agent_name'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'agent_phone'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'architecture_style'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'available_showing'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'basement'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'bathrooms'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'beds'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'brokerage'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'brokerage_email'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'brokerage_external_id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'brokerage_logo_url'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'brokerage_name'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'brokerage_phone'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'brokerage_website_url'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'building_type'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'building_utilities'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'city'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'cooling_systems'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'country'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'cover_public_url'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'created_at'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'description'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'disclose_address'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'exterior_types'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'external_id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'external_url'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'fees'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'fireplaces'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'floors'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'floors_number'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'foreclosure'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'full_baths'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'garage'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'geocoded'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'half_baths'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'has_attic'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'has_barbecue_area'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'has_basement'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'has_ceiling_fan'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'has_deck'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'has_disabled_access'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'has_dock'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'has_doorman'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'has_double_pane_windows'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'has_elevator'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'has_fireplace'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'has_garage'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'has_garden'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'has_gated_entry'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'has_green_house'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'has_hot_tub_spa'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'has_jetted_bath_tub'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'has_mother_in_law'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'has_patio'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'has_pond'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'has_pool'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'has_porch'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'has_rv_parking'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'has_sauna'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'has_security_system'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'has_skylight'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'has_sports_court'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'has_sprinkler_system'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'has_vaulted_ceiling'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'has_wet_bar'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'heating_fuels'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'heating_systems'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'images'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'intercom'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'is_cable_ready'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'is_new_construction'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'is_waterfront'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'is_wired'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'latitude'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'lead_routing_email'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'listing_category'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'listing_date'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'listing_status'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'listing_title'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'listing_type'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'listing_url'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'living_area'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'location'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'location_ids'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'longitude'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'lower_location_id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'mls_id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'mls_name'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'mls_number'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'modified_at'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'num_floors'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'one_quarter_bathrooms'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'parking'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'parking_types'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'partial_bathrooms'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'participant'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'patio'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'pool'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'postal'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'price_cents'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'price_cents_sqft'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'property_sub_type'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'property_type'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'provider_category'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'provider_name'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'provider_url'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'province'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'roof'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'roof_types'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'room_count'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rooms'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'school'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'scoring'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'showing_date'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'status'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'tags'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'tax_amount'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'three_quarter_bathrooms'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'unit_number'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'updated_at'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'utilities'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'water'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'water_heater'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'year_built'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: select() takes from 2 to 3 positional arguments but 135 were given"
     ]
    }
   ],
   "source": [
    "df.select('_id','address_number','address_street','agent_avatar_url','agent_email','agent_external_id','agent_fname','agent_id','agent_key','agent_lname','agent_name','agent_phone','architecture_style','available_showing','basement','bathrooms','beds','brokerage','brokerage_email','brokerage_external_id','brokerage_logo_url','brokerage_name','brokerage_phone','brokerage_website_url','building_type','building_utilities','city','cooling_systems','country','cover_public_url','created_at','description','disclose_address','exterior_types','external_id','external_url','fees','fireplaces','floors','floors_number','foreclosure','full_baths','garage','geocoded','half_baths','has_attic','has_barbecue_area','has_basement','has_ceiling_fan','has_deck','has_disabled_access','has_dock','has_doorman','has_double_pane_windows','has_elevator','has_fireplace','has_garage','has_garden','has_gated_entry','has_green_house','has_hot_tub_spa','has_jetted_bath_tub','has_mother_in_law','has_patio','has_pond','has_pool','has_porch','has_rv_parking','has_sauna','has_security_system','has_skylight','has_sports_court','has_sprinkler_system','has_vaulted_ceiling','has_wet_bar','heating_fuels','heating_systems','images','intercom','is_cable_ready','is_new_construction','is_waterfront','is_wired','latitude','lead_routing_email','listing_category','listing_date','listing_status','listing_title','listing_type','listing_url','living_area','location','location_ids','longitude','lower_location_id','mls_id','mls_name','mls_number','modified_at','num_floors','one_quarter_bathrooms','parking','parking_types','partial_bathrooms','participant','patio','pool','postal','price_cents','price_cents_sqft','property_sub_type','property_type','provider_category','provider_name','provider_url','province','roof','roof_types','room_count','rooms','school','scoring','showing_date','status','tags','tax_amount','three_quarter_bathrooms','unit_number','updated_at','utilities','water','water_heater','year_built').show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       NaN\n",
       "1       NaN\n",
       "2       NaN\n",
       "3       NaN\n",
       "4       NaN\n",
       "5       NaN\n",
       "6       NaN\n",
       "7       NaN\n",
       "8       NaN\n",
       "9       NaN\n",
       "10      NaN\n",
       "11      NaN\n",
       "12      NaN\n",
       "13      NaN\n",
       "14      NaN\n",
       "15      NaN\n",
       "16      NaN\n",
       "17      NaN\n",
       "18      NaN\n",
       "19      NaN\n",
       "20      NaN\n",
       "21      NaN\n",
       "22      NaN\n",
       "23      NaN\n",
       "24      NaN\n",
       "25      NaN\n",
       "26      NaN\n",
       "27      NaN\n",
       "28      NaN\n",
       "29      NaN\n",
       "         ..\n",
       "99970   NaN\n",
       "99971   NaN\n",
       "99972   NaN\n",
       "99973   NaN\n",
       "99974   NaN\n",
       "99975   NaN\n",
       "99976   NaN\n",
       "99977   NaN\n",
       "99978   NaN\n",
       "99979   NaN\n",
       "99980   NaN\n",
       "99981   NaN\n",
       "99982   NaN\n",
       "99983   NaN\n",
       "99984   NaN\n",
       "99985   NaN\n",
       "99986   NaN\n",
       "99987   NaN\n",
       "99988   NaN\n",
       "99989   NaN\n",
       "99990   NaN\n",
       "99991   NaN\n",
       "99992   NaN\n",
       "99993   NaN\n",
       "99994   NaN\n",
       "99995   NaN\n",
       "99996   NaN\n",
       "99997   NaN\n",
       "99998   NaN\n",
       "99999   NaN\n",
       "Name: total_sqft, Length: 100000, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['total_sqft']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema='root\n",
    " |-- _id: struct (nullable = true)\n",
    " |    |-- oid: string (nullable = true)\n",
    " |-- address_number: string (nullable = true)\n",
    " |-- address_street: string (nullable = true)\n",
    " |-- agent_avatar_url: null (nullable = true)\n",
    " |-- agent_email: string (nullable = true)\n",
    " |-- agent_external_id: string (nullable = true)\n",
    " |-- agent_fname: string (nullable = true)\n",
    " |-- agent_id: string (nullable = true)\n",
    " |-- agent_key: string (nullable = true)\n",
    " |-- agent_lname: string (nullable = true)\n",
    " |-- agent_name: string (nullable = true)\n",
    " |-- agent_phone: string (nullable = true)\n",
    " |-- architecture_style: string (nullable = true)\n",
    " |-- available_showing: boolean (nullable = true)\n",
    " |-- basement: boolean (nullable = true)\n",
    " |-- bathrooms: double (nullable = true)\n",
    " |-- beds: integer (nullable = true)\n",
    " |-- brokerage: struct (nullable = true)\n",
    " |    |-- address: struct (nullable = true)\n",
    " |    |    |-- commons:City: string (nullable = true)\n",
    " |    |    |-- commons:Country: string (nullable = true)\n",
    " |    |    |-- commons:FullStreetAddress: string (nullable = true)\n",
    " |    |    |-- commons:PostalCode: string (nullable = true)\n",
    " |    |    |-- commons:StateOrProvince: string (nullable = true)\n",
    " |    |    |-- commons:UnitNumber: string (nullable = true)\n",
    " |    |    |-- commons:address-preference-order: string (nullable = true)\n",
    " |    |    |-- commons:preference-order: string (nullable = true)\n",
    " |    |-- logo_url: string (nullable = true)\n",
    " |    |-- name: string (nullable = true)\n",
    " |    |-- phone: string (nullable = true)\n",
    " |    |-- website_url: string (nullable = true)\n",
    " |-- brokerage_email: string (nullable = true)\n",
    " |-- brokerage_external_id: string (nullable = true)\n",
    " |-- brokerage_logo_url: string (nullable = true)\n",
    " |-- brokerage_name: string (nullable = true)\n",
    " |-- brokerage_phone: string (nullable = true)\n",
    " |-- brokerage_website_url: string (nullable = true)\n",
    " |-- building_sqft: double (nullable = true)\n",
    " |-- building_type: string (nullable = true)\n",
    " |-- building_utilities: string (nullable = true)\n",
    " |-- city: string (nullable = true)\n",
    " |-- cooling_systems: array (nullable = true)\n",
    " |    |-- element: string (containsNull = true)\n",
    " |-- country: string (nullable = true)\n",
    " |-- cover_public_url: string (nullable = true)\n",
    " |-- created_at: timestamp (nullable = true)\n",
    " |-- description: string (nullable = true)\n",
    " |-- disclose_address: string (nullable = true)\n",
    " |-- exterior_types: array (nullable = true)\n",
    " |    |-- element: string (containsNull = true)\n",
    " |-- external_id: string (nullable = true)\n",
    " |-- external_type: struct (nullable = true)\n",
    " |    |-- symbol: string (nullable = true)\n",
    " |-- external_url: string (nullable = true)\n",
    " |-- fees: null (nullable = true)\n",
    " |-- fireplaces: string (nullable = true)\n",
    " |-- floors: string (nullable = true)\n",
    " |-- floors_number: double (nullable = true)\n",
    " |-- foreclosure: boolean (nullable = true)\n",
    " |-- full_baths: integer (nullable = true)\n",
    " |-- furnished: null (nullable = true)\n",
    " |-- garage: boolean (nullable = true)\n",
    " |-- geocoded: boolean (nullable = true)\n",
    " |-- half_baths: integer (nullable = true)\n",
    " |-- has_attic: null (nullable = true)\n",
    " |-- has_barbecue_area: boolean (nullable = true)\n",
    " |-- has_basement: boolean (nullable = true)\n",
    " |-- has_ceiling_fan: boolean (nullable = true)\n",
    " |-- has_deck: boolean (nullable = true)\n",
    " |-- has_disabled_access: boolean (nullable = true)\n",
    " |-- has_dock: boolean (nullable = true)\n",
    " |-- has_doorman: boolean (nullable = true)\n",
    " |-- has_double_pane_windows: boolean (nullable = true)\n",
    " |-- has_elevator: boolean (nullable = true)\n",
    " |-- has_fireplace: boolean (nullable = true)\n",
    " |-- has_garage: boolean (nullable = true)\n",
    " |-- has_garden: boolean (nullable = true)\n",
    " |-- has_gated_entry: boolean (nullable = true)\n",
    " |-- has_green_house: boolean (nullable = true)\n",
    " |-- has_hot_tub_spa: boolean (nullable = true)\n",
    " |-- has_jetted_bath_tub: boolean (nullable = true)\n",
    " |-- has_mother_in_law: boolean (nullable = true)\n",
    " |-- has_patio: boolean (nullable = true)\n",
    " |-- has_pond: boolean (nullable = true)\n",
    " |-- has_pool: boolean (nullable = true)\n",
    " |-- has_porch: boolean (nullable = true)\n",
    " |-- has_rv_parking: boolean (nullable = true)\n",
    " |-- has_sauna: boolean (nullable = true)\n",
    " |-- has_security_system: boolean (nullable = true)\n",
    " |-- has_skylight: boolean (nullable = true)\n",
    " |-- has_sports_court: boolean (nullable = true)\n",
    " |-- has_sprinkler_system: boolean (nullable = true)\n",
    " |-- has_vaulted_ceiling: boolean (nullable = true)\n",
    " |-- has_wet_bar: boolean (nullable = true)\n",
    " |-- heating_fuels: array (nullable = true)\n",
    " |    |-- element: string (containsNull = true)\n",
    " |-- heating_systems: array (nullable = true)\n",
    " |    |-- element: string (containsNull = true)\n",
    " |-- images: array (nullable = true)\n",
    " |    |-- element: struct (containsNull = true)\n",
    " |    |    |-- _id: struct (nullable = true)\n",
    " |    |    |    |-- oid: string (nullable = true)\n",
    " |    |    |-- created_at: timestamp (nullable = true)\n",
    " |    |    |-- modified_at: timestamp (nullable = true)\n",
    " |    |    |-- original_url: string (nullable = true)\n",
    " |    |    |-- sha1: string (nullable = true)\n",
    " |    |    |-- updated_at: timestamp (nullable = true)\n",
    " |    |    |-- url: string (nullable = true)\n",
    " |-- intercom: boolean (nullable = true)\n",
    " |-- is_cable_ready: boolean (nullable = true)\n",
    " |-- is_new_construction: boolean (nullable = true)\n",
    " |-- is_waterfront: boolean (nullable = true)\n",
    " |-- is_wired: boolean (nullable = true)\n",
    " |-- latitude: double (nullable = true)\n",
    " |-- lead_routing_email: string (nullable = true)\n",
    " |-- listing_category: string (nullable = true)\n",
    " |-- listing_date: timestamp (nullable = true)\n",
    " |-- listing_status: struct (nullable = true)\n",
    " |    |-- symbol: string (nullable = true)\n",
    " |-- listing_title: string (nullable = true)\n",
    " |-- listing_type: string (nullable = true)\n",
    " |-- listing_url: string (nullable = true)\n",
    " |-- living_area: string (nullable = true)\n",
    " |-- location: struct (nullable = true)\n",
    " |    |-- type: string (nullable = true)\n",
    " |    |-- coordinates: array (nullable = true)\n",
    " |    |    |-- element: double (containsNull = true)\n",
    " |-- location_ids: array (nullable = true)\n",
    " |    |-- element: struct (containsNull = true)\n",
    " |    |    |-- oid: string (nullable = true)\n",
    " |-- longitude: double (nullable = true)\n",
    " |-- lot_sqft: double (nullable = true)\n",
    " |-- lower_location_id: struct (nullable = true)\n",
    " |    |-- oid: string (nullable = true)\n",
    " |-- main_level_sqft: double (nullable = true)\n",
    " |-- mls_id: string (nullable = true)\n",
    " |-- mls_name: string (nullable = true)\n",
    " |-- mls_number: string (nullable = true)\n",
    " |-- modified_at: timestamp (nullable = true)\n",
    " |-- num_floors: double (nullable = true)\n",
    " |-- one_quarter_bathrooms: integer (nullable = true)\n",
    " |-- open_house_count: null (nullable = true)\n",
    " |-- parking: string (nullable = true)\n",
    " |-- parking_types: array (nullable = true)\n",
    " |    |-- element: string (containsNull = true)\n",
    " |-- partial_bathrooms: integer (nullable = true)\n",
    " |-- participant: struct (nullable = true)\n",
    " |    |-- email: string (nullable = true)\n",
    " |    |-- first_name: string (nullable = true)\n",
    " |    |-- id: string (nullable = true)\n",
    " |    |-- key: string (nullable = true)\n",
    " |    |-- last_name: string (nullable = true)\n",
    " |    |-- office_phone: string (nullable = true)\n",
    " |    |-- primary_contact_phone: string (nullable = true)\n",
    " |    |-- website_url: string (nullable = true)\n",
    " |-- patio: boolean (nullable = true)\n",
    " |-- pool: boolean (nullable = true)\n",
    " |-- postal: string (nullable = true)\n",
    " |-- price_cents: integer (nullable = true)\n",
    " |-- price_cents_sqft: integer (nullable = true)\n",
    " |-- property_sub_type: string (nullable = true)\n",
    " |-- property_type: string (nullable = true)\n",
    " |-- provider_category: string (nullable = true)\n",
    " |-- provider_name: string (nullable = true)\n",
    " |-- provider_url: string (nullable = true)\n",
    " |-- province: string (nullable = true)\n",
    " |-- roof: string (nullable = true)\n",
    " |-- roof_types: array (nullable = true)\n",
    " |    |-- element: string (containsNull = true)\n",
    " |-- room_count: integer (nullable = true)\n",
    " |-- rooms: array (nullable = true)\n",
    " |    |-- element: string (containsNull = true)\n",
    " |-- school: array (nullable = true)\n",
    " |    |-- element: string (containsNull = true)\n",
    " |-- scoring: null (nullable = true)\n",
    " |-- showing_date: timestamp (nullable = true)\n",
    " |-- state: null (nullable = true)\n",
    " |-- status: struct (nullable = true)\n",
    " |    |-- symbol: string (nullable = true)\n",
    " |-- stories: null (nullable = true)\n",
    " |-- tags: array (nullable = true)\n",
    " |    |-- element: struct (containsNull = true)\n",
    " |    |    |-- symbol: string (nullable = true)\n",
    " |-- tax_amount: string (nullable = true)\n",
    " |-- three_quarter_bathrooms: integer (nullable = true)\n",
    " |-- total_sqft: double (nullable = true)\n",
    " |-- unit_number: string (nullable = true)\n",
    " |-- updated_at: timestamp (nullable = true)\n",
    " |-- utilities: string (nullable = true)\n",
    " |-- water: string (nullable = true)\n",
    " |-- water_heater: string (nullable = true)\n",
    "\n",
    "    |-- year_built: integer (nullable = true)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-83-b89e53631f67>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-83-b89e53631f67>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    'main_level_sqft'  - Cannot cast STRING into a DoubleType (value: BsonString{value='1398'})\u001b[0m\n\u001b[0m                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "##Following are the Schema type mismatches when Spark loads property data\n",
    "\n",
    "'total_sqft' - Cannot cast STRING into a DoubleType (value: BsonString{value='1398'})\n",
    "'building_sqft' - Cannot cast STRING into a DoubleType (value: BsonString{value=''})\n",
    "'lot_sqft' - Cannot cast STRING into a DoubleType (value: BsonString{value='NA'})\n",
    "'external_url'- Cannot cast STRING into a DoubleType (value: BsonString{value='1398'})\n",
    "'furnished' - Cannot cast BOOLEAN into a NullType (value: BsonBoolean{value=false})\n",
    "'main_level_sqft'  - Cannot cast STRING into a DoubleType (value: BsonString{value='1398'})\n",
    "'open_house_count' - Cannot cast STRING into a NullType (value: BsonString{value=''})    \n",
    "'state' - Cannot cast STRING into a NullType (value: BsonString{value='active'})\n",
    "'stories' - Cannot cast STRING into a IntegerType (value: BsonString{value=''})\n",
    "'total_sqft'- Cannot cast STRING into a DoubleType (value: BsonString{value='1398'})\n",
    "'available_showing' -     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
